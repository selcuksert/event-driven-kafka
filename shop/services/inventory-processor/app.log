2020-06-24 17:27:08.186  INFO 18444 --- [RMI TCP Connection(15)-127.0.0.1] inMXBeanRegistrar$SpringApplicationAdmin : Application shutdown requested.
2020-06-24 17:27:08.196 DEBUG 18444 --- [RMI TCP Connection(15)-127.0.0.1] ConfigServletWebServerApplicationContext : Closing org.springframework.boot.web.servlet.context.AnnotationConfigServletWebServerApplicationContext@466276d8, started on Wed Jun 24 17:20:00 EET 2020
2020-06-24 17:27:08.471  INFO 18444 --- [RMI TCP Connection(15)-127.0.0.1] org.apache.kafka.streams.KafkaStreams    : stream-client [sinkItem-applicationId-bfb91902-e97f-4823-806f-ee64e3d02356] State transition from RUNNING to PENDING_SHUTDOWN
2020-06-24 17:27:08.472  INFO 18444 --- [kafka-streams-close-thread] o.a.k.s.p.internals.StreamThread         : stream-thread [sinkItem-applicationId-bfb91902-e97f-4823-806f-ee64e3d02356-StreamThread-1] Informed to shut down
2020-06-24 17:27:08.472  INFO 18444 --- [kafka-streams-close-thread] o.a.k.s.p.internals.StreamThread         : stream-thread [sinkItem-applicationId-bfb91902-e97f-4823-806f-ee64e3d02356-StreamThread-1] State transition from RUNNING to PENDING_SHUTDOWN
2020-06-24 17:27:08.535  INFO 18444 --- [sinkItem-applicationId-bfb91902-e97f-4823-806f-ee64e3d02356-StreamThread-1] o.a.k.s.p.internals.StreamThread         : stream-thread [sinkItem-applicationId-bfb91902-e97f-4823-806f-ee64e3d02356-StreamThread-1] Shutting down
2020-06-24 17:27:08.539  INFO 18444 --- [sinkItem-applicationId-bfb91902-e97f-4823-806f-ee64e3d02356-StreamThread-1] o.a.k.clients.consumer.KafkaConsumer     : [Consumer clientId=sinkItem-applicationId-bfb91902-e97f-4823-806f-ee64e3d02356-StreamThread-1-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions
2020-06-24 17:27:08.539  INFO 18444 --- [sinkItem-applicationId-bfb91902-e97f-4823-806f-ee64e3d02356-StreamThread-1] o.a.k.clients.producer.KafkaProducer     : [Producer clientId=sinkItem-applicationId-bfb91902-e97f-4823-806f-ee64e3d02356-StreamThread-1-producer] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2020-06-24 17:27:08.554  INFO 18444 --- [sinkItem-applicationId-bfb91902-e97f-4823-806f-ee64e3d02356-StreamThread-1] o.a.k.s.p.internals.StreamThread         : stream-thread [sinkItem-applicationId-bfb91902-e97f-4823-806f-ee64e3d02356-StreamThread-1] State transition from PENDING_SHUTDOWN to DEAD
2020-06-24 17:27:08.554  INFO 18444 --- [sinkItem-applicationId-bfb91902-e97f-4823-806f-ee64e3d02356-StreamThread-1] o.a.k.s.p.internals.StreamThread         : stream-thread [sinkItem-applicationId-bfb91902-e97f-4823-806f-ee64e3d02356-StreamThread-1] Shutdown complete
2020-06-24 17:27:08.557  INFO 18444 --- [kafka-streams-close-thread] org.apache.kafka.streams.KafkaStreams    : stream-client [sinkItem-applicationId-bfb91902-e97f-4823-806f-ee64e3d02356] State transition from PENDING_SHUTDOWN to NOT_RUNNING
2020-06-24 17:27:08.557  INFO 18444 --- [RMI TCP Connection(15)-127.0.0.1] org.apache.kafka.streams.KafkaStreams    : stream-client [sinkItem-applicationId-bfb91902-e97f-4823-806f-ee64e3d02356] Streams client stopped completely
2020-06-24 17:27:08.558  INFO 18444 --- [RMI TCP Connection(15)-127.0.0.1] org.apache.kafka.streams.KafkaStreams    : stream-client [processItem-applicationId-0f52f315-57d3-4f9e-9f57-9a138d674f7f] State transition from RUNNING to PENDING_SHUTDOWN
2020-06-24 17:27:08.558  INFO 18444 --- [kafka-streams-close-thread] o.a.k.s.p.internals.StreamThread         : stream-thread [processItem-applicationId-0f52f315-57d3-4f9e-9f57-9a138d674f7f-StreamThread-1] Informed to shut down
2020-06-24 17:27:08.558  INFO 18444 --- [kafka-streams-close-thread] o.a.k.s.p.internals.StreamThread         : stream-thread [processItem-applicationId-0f52f315-57d3-4f9e-9f57-9a138d674f7f-StreamThread-1] State transition from RUNNING to PENDING_SHUTDOWN
2020-06-24 17:27:08.590  INFO 18444 --- [processItem-applicationId-0f52f315-57d3-4f9e-9f57-9a138d674f7f-StreamThread-1] o.a.k.s.p.internals.StreamThread         : stream-thread [processItem-applicationId-0f52f315-57d3-4f9e-9f57-9a138d674f7f-StreamThread-1] Shutting down
2020-06-24 17:27:08.591  INFO 18444 --- [processItem-applicationId-0f52f315-57d3-4f9e-9f57-9a138d674f7f-StreamThread-1] o.a.k.clients.consumer.KafkaConsumer     : [Consumer clientId=processItem-applicationId-0f52f315-57d3-4f9e-9f57-9a138d674f7f-StreamThread-1-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions
2020-06-24 17:27:08.591  INFO 18444 --- [processItem-applicationId-0f52f315-57d3-4f9e-9f57-9a138d674f7f-StreamThread-1] o.a.k.clients.producer.KafkaProducer     : [Producer clientId=processItem-applicationId-0f52f315-57d3-4f9e-9f57-9a138d674f7f-StreamThread-1-producer] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2020-06-24 17:27:08.597  INFO 18444 --- [processItem-applicationId-0f52f315-57d3-4f9e-9f57-9a138d674f7f-StreamThread-1] o.a.k.s.p.internals.StreamThread         : stream-thread [processItem-applicationId-0f52f315-57d3-4f9e-9f57-9a138d674f7f-StreamThread-1] State transition from PENDING_SHUTDOWN to DEAD
2020-06-24 17:27:08.597  INFO 18444 --- [processItem-applicationId-0f52f315-57d3-4f9e-9f57-9a138d674f7f-StreamThread-1] o.a.k.s.p.internals.StreamThread         : stream-thread [processItem-applicationId-0f52f315-57d3-4f9e-9f57-9a138d674f7f-StreamThread-1] Shutdown complete
2020-06-24 17:27:08.599  INFO 18444 --- [kafka-streams-close-thread] org.apache.kafka.streams.KafkaStreams    : stream-client [processItem-applicationId-0f52f315-57d3-4f9e-9f57-9a138d674f7f] State transition from PENDING_SHUTDOWN to NOT_RUNNING
2020-06-24 17:27:08.599  INFO 18444 --- [RMI TCP Connection(15)-127.0.0.1] org.apache.kafka.streams.KafkaStreams    : stream-client [processItem-applicationId-0f52f315-57d3-4f9e-9f57-9a138d674f7f] Streams client stopped completely
2020-06-24 17:27:08.600  INFO 18444 --- [RMI TCP Connection(15)-127.0.0.1] org.apache.kafka.streams.KafkaStreams    : stream-client [sinkInventory-applicationId-38ca4d26-c07b-40c2-92d6-5f217853d419] State transition from ERROR to PENDING_SHUTDOWN
2020-06-24 17:27:08.600  INFO 18444 --- [kafka-streams-close-thread] o.a.k.s.p.internals.StreamThread         : stream-thread [sinkInventory-applicationId-38ca4d26-c07b-40c2-92d6-5f217853d419-StreamThread-1] Informed to shut down
2020-06-24 17:27:08.602  INFO 18444 --- [kafka-streams-close-thread] org.apache.kafka.streams.KafkaStreams    : stream-client [sinkInventory-applicationId-38ca4d26-c07b-40c2-92d6-5f217853d419] State transition from PENDING_SHUTDOWN to NOT_RUNNING
2020-06-24 17:27:08.602  INFO 18444 --- [RMI TCP Connection(15)-127.0.0.1] org.apache.kafka.streams.KafkaStreams    : stream-client [sinkInventory-applicationId-38ca4d26-c07b-40c2-92d6-5f217853d419] Streams client stopped completely
2020-06-24 17:27:08.607  INFO 18444 --- [RMI TCP Connection(15)-127.0.0.1] o.a.k.s.p.internals.StateDirectory       : stream-thread [RMI TCP Connection(15)-127.0.0.1] Deleting obsolete state directory 0_0 for task 0_0 as 424465ms has elapsed (cleanup delay is 0ms).
2020-06-24 17:27:08.621  INFO 18444 --- [RMI TCP Connection(15)-127.0.0.1] o.a.k.s.p.internals.StateDirectory       : stream-thread [RMI TCP Connection(15)-127.0.0.1] Deleting obsolete state directory 0_1 for task 0_1 as 424357ms has elapsed (cleanup delay is 0ms).
2020-06-24 17:27:08.634  INFO 18444 --- [RMI TCP Connection(15)-127.0.0.1] o.s.i.endpoint.EventDrivenConsumer       : Removing {logging-channel-adapter:_org.springframework.integration.errorLogger} as a subscriber to the 'errorChannel' channel
2020-06-24 17:27:08.635  INFO 18444 --- [RMI TCP Connection(15)-127.0.0.1] o.s.i.channel.PublishSubscribeChannel    : Channel 'application.errorChannel' has 0 subscriber(s).
2020-06-24 17:27:08.635  INFO 18444 --- [RMI TCP Connection(15)-127.0.0.1] o.s.i.endpoint.EventDrivenConsumer       : stopped bean '_org.springframework.integration.errorLogger'
2020-06-24 17:27:08.637  INFO 18444 --- [RMI TCP Connection(15)-127.0.0.1] o.s.s.c.ThreadPoolTaskScheduler          : Shutting down ExecutorService 'taskScheduler'
2020-06-24 17:27:08.639  INFO 18444 --- [RMI TCP Connection(15)-127.0.0.1] o.s.s.concurrent.ThreadPoolTaskExecutor  : Shutting down ExecutorService 'applicationTaskExecutor'
2020-06-24 17:27:08.640  INFO 18444 --- [RMI TCP Connection(15)-127.0.0.1] o.s.i.monitor.IntegrationMBeanExporter   : Summary on shutdown: nullChannel
2020-06-24 17:27:08.640  INFO 18444 --- [RMI TCP Connection(15)-127.0.0.1] o.s.i.monitor.IntegrationMBeanExporter   : Summary on shutdown: bean 'errorChannel'
2020-06-24 17:27:08.640  INFO 18444 --- [RMI TCP Connection(15)-127.0.0.1] o.s.i.monitor.IntegrationMBeanExporter   : Summary on shutdown: bean 'output'
2020-06-24 17:27:08.640  INFO 18444 --- [RMI TCP Connection(15)-127.0.0.1] o.s.i.monitor.IntegrationMBeanExporter   : Summary on shutdown: bean '_org.springframework.integration.errorLogger.handler' for component '_org.springframework.integration.errorLogger'
2020-06-24 17:27:20.445 DEBUG 18164 --- [main] .c.l.ClasspathLoggingApplicationListener : Application started with classpath: unknown
2020-06-24 17:27:20.498  INFO 18164 --- [main] c.t.c.s.s.i.InventoryApplication         : Starting InventoryApplication on TN40384 with PID 18164 (Q:\dev\projects\inhouse\event-driven-kafka\shop\services\inventory\target\classes started by S_SERT1 in Q:\dev\projects\inhouse\event-driven-kafka\shop\services\inventory)
2020-06-24 17:27:20.498  INFO 18164 --- [main] c.t.c.s.s.i.InventoryApplication         : No active profile set, falling back to default profiles: default
2020-06-24 17:27:20.499 DEBUG 18164 --- [main] o.s.boot.SpringApplication               : Loading source class com.turkishairlines.concepts.shop.services.inventory.InventoryApplication
2020-06-24 17:27:20.537 DEBUG 18164 --- [main] o.s.b.c.c.ConfigFileApplicationListener  : Loaded config file 'file:/Q:/dev/projects/inhouse/event-driven-kafka/shop/services/inventory/target/classes/application.yml' (classpath:/application.yml)
2020-06-24 17:27:20.537 DEBUG 18164 --- [main] ConfigServletWebServerApplicationContext : Refreshing org.springframework.boot.web.servlet.context.AnnotationConfigServletWebServerApplicationContext@19e4fcac
2020-06-24 17:27:21.068 DEBUG 18164 --- [main] o.s.b.a.AutoConfigurationPackages        : @EnableAutoConfiguration was declared on a class in the package 'com.turkishairlines.concepts.shop.services.inventory'. Automatic @Repository and @Entity scanning is enabled.
2020-06-24 17:27:21.270  INFO 18164 --- [main] faultConfiguringBeanFactoryPostProcessor : No bean named 'errorChannel' has been explicitly defined. Therefore, a default PublishSubscribeChannel will be created.
2020-06-24 17:27:21.274  INFO 18164 --- [main] faultConfiguringBeanFactoryPostProcessor : No bean named 'taskScheduler' has been explicitly defined. Therefore, a default ThreadPoolTaskScheduler will be created.
2020-06-24 17:27:21.278  INFO 18164 --- [main] faultConfiguringBeanFactoryPostProcessor : No bean named 'integrationHeaderChannelRegistry' has been explicitly defined. Therefore, a default DefaultHeaderChannelRegistry will be created.
2020-06-24 17:27:21.312  INFO 18164 --- [main] trationDelegate$BeanPostProcessorChecker : Bean 'integrationChannelResolver' of type [org.springframework.integration.support.channel.BeanFactoryChannelResolver] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
2020-06-24 17:27:21.315  INFO 18164 --- [main] trationDelegate$BeanPostProcessorChecker : Bean 'integrationDisposableAutoCreatedBeans' of type [org.springframework.integration.config.annotation.Disposables] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
2020-06-24 17:27:21.322  INFO 18164 --- [main] trationDelegate$BeanPostProcessorChecker : Bean 'org.springframework.integration.config.IntegrationManagementConfiguration' of type [org.springframework.integration.config.IntegrationManagementConfiguration] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
2020-06-24 17:27:21.325  INFO 18164 --- [main] trationDelegate$BeanPostProcessorChecker : Bean 'org.springframework.boot.autoconfigure.integration.IntegrationAutoConfiguration$IntegrationJmxConfiguration' of type [org.springframework.boot.autoconfigure.integration.IntegrationAutoConfiguration$IntegrationJmxConfiguration] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
2020-06-24 17:27:21.330  INFO 18164 --- [main] trationDelegate$BeanPostProcessorChecker : Bean 'org.springframework.boot.autoconfigure.jmx.JmxAutoConfiguration' of type [org.springframework.boot.autoconfigure.jmx.JmxAutoConfiguration] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
2020-06-24 17:27:21.333  INFO 18164 --- [main] trationDelegate$BeanPostProcessorChecker : Bean 'mbeanServer' of type [com.sun.jmx.mbeanserver.JmxMBeanServer] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
2020-06-24 17:27:21.541 DEBUG 18164 --- [main] .s.b.w.e.t.TomcatServletWebServerFactory : Code archive: Q:\dev\maven\maven-repo\org\springframework\boot\spring-boot\2.3.1.RELEASE\spring-boot-2.3.1.RELEASE.jar
2020-06-24 17:27:21.541 DEBUG 18164 --- [main] .s.b.w.e.t.TomcatServletWebServerFactory : Code archive: Q:\dev\maven\maven-repo\org\springframework\boot\spring-boot\2.3.1.RELEASE\spring-boot-2.3.1.RELEASE.jar
2020-06-24 17:27:21.542 DEBUG 18164 --- [main] .s.b.w.e.t.TomcatServletWebServerFactory : None of the document roots [src/main/webapp, public, static] point to a directory and will be ignored.
2020-06-24 17:27:21.561  INFO 18164 --- [main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port(s): 9000 (http)
2020-06-24 17:27:21.569  INFO 18164 --- [main] o.apache.catalina.core.StandardService   : Starting service [Tomcat]
2020-06-24 17:27:21.569  INFO 18164 --- [main] org.apache.catalina.core.StandardEngine  : Starting Servlet engine: [Apache Tomcat/9.0.36]
2020-06-24 17:27:21.947  INFO 18164 --- [main] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext
2020-06-24 17:27:21.947 DEBUG 18164 --- [main] w.s.c.ServletWebServerApplicationContext : Published root WebApplicationContext as ServletContext attribute with name [org.springframework.web.context.WebApplicationContext.ROOT]
2020-06-24 17:27:21.947  INFO 18164 --- [main] w.s.c.ServletWebServerApplicationContext : Root WebApplicationContext: initialization completed in 1410 ms
2020-06-24 17:27:21.969 DEBUG 18164 --- [main] o.s.b.w.s.ServletContextInitializerBeans : Mapping filters: characterEncodingFilter urls=[/*] order=-2147483648, formContentFilter urls=[/*] order=-9900, requestContextFilter urls=[/*] order=-105
2020-06-24 17:27:21.969 DEBUG 18164 --- [main] o.s.b.w.s.ServletContextInitializerBeans : Mapping servlets: dispatcherServlet urls=[/]
2020-06-24 17:27:21.983 DEBUG 18164 --- [main] o.s.b.w.s.f.OrderedRequestContextFilter  : Filter 'requestContextFilter' configured for use
2020-06-24 17:27:21.984 DEBUG 18164 --- [main] s.b.w.s.f.OrderedCharacterEncodingFilter : Filter 'characterEncodingFilter' configured for use
2020-06-24 17:27:21.984 DEBUG 18164 --- [main] o.s.b.w.s.f.OrderedFormContentFilter     : Filter 'formContentFilter' configured for use
2020-06-24 17:27:22.300  INFO 18164 --- [main] o.s.s.concurrent.ThreadPoolTaskExecutor  : Initializing ExecutorService 'applicationTaskExecutor'
2020-06-24 17:27:22.308 DEBUG 18164 --- [main] s.w.s.m.m.a.RequestMappingHandlerAdapter : ControllerAdvice beans: 0 @ModelAttribute, 0 @InitBinder, 1 RequestBodyAdvice, 1 ResponseBodyAdvice
2020-06-24 17:27:22.346 DEBUG 18164 --- [main] s.w.s.m.m.a.RequestMappingHandlerMapping : 4 mappings in 'requestMappingHandlerMapping'
2020-06-24 17:27:22.367 DEBUG 18164 --- [main] o.s.w.s.handler.SimpleUrlHandlerMapping  : Patterns [/webjars/**, /**] in 'resourceHandlerMapping'
2020-06-24 17:27:22.375 DEBUG 18164 --- [main] .m.m.a.ExceptionHandlerExceptionResolver : ControllerAdvice beans: 0 @ExceptionHandler, 1 ResponseBodyAdvice
2020-06-24 17:27:22.389 DEBUG 18164 --- [main] inMXBeanRegistrar$SpringApplicationAdmin : Application Admin MBean registered with name 'org.springframework.boot:type=Admin,name=SpringApplication'
2020-06-24 17:27:22.483  INFO 18164 --- [main] o.s.s.c.ThreadPoolTaskScheduler          : Initializing ExecutorService 'taskScheduler'
2020-06-24 17:27:22.493  INFO 18164 --- [main] onConfiguration$FunctionBindingRegistrar : Functional binding is disabled due to the presense of @EnableBinding annotation in your configuration
2020-06-24 17:27:22.497  INFO 18164 --- [main] o.s.c.f.c.c.SimpleFunctionRegistry       : Looking up function 'sinkItem| sinkInventory| processItem' with acceptedOutputTypes: []
2020-06-24 17:27:22.565  INFO 18164 --- [main] .k.s.AbstractKafkaStreamsBinderProcessor : Binder Generated Kafka Streams Application ID: processItem-applicationId
2020-06-24 17:27:22.566  INFO 18164 --- [main] .k.s.AbstractKafkaStreamsBinderProcessor : Use the binder generated application ID only for development and testing. 
2020-06-24 17:27:22.566  INFO 18164 --- [main] .k.s.AbstractKafkaStreamsBinderProcessor : For production deployments, please consider explicitly setting an application ID using a configuration property.
2020-06-24 17:27:22.566  INFO 18164 --- [main] .k.s.AbstractKafkaStreamsBinderProcessor : The generated applicationID is static and will be preserved over application restarts.
2020-06-24 17:27:22.595  INFO 18164 --- [main] i.c.k.s.KafkaAvroSerializerConfig        : KafkaAvroSerializerConfig values: 
	bearer.auth.token = [hidden]
	proxy.port = -1
	schema.reflection = false
	auto.register.schemas = true
	max.schemas.per.subject = 1000
	basic.auth.credentials.source = URL
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	schema.registry.url = [http://localhost:8081]
	basic.auth.user.info = [hidden]
	proxy.host = 
	schema.registry.basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2020-06-24 17:27:22.604  INFO 18164 --- [main] i.c.k.s.KafkaAvroDeserializerConfig      : KafkaAvroDeserializerConfig values: 
	bearer.auth.token = [hidden]
	proxy.port = -1
	schema.reflection = false
	auto.register.schemas = true
	max.schemas.per.subject = 1000
	basic.auth.credentials.source = URL
	specific.avro.reader = false
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	schema.registry.url = [http://localhost:8081]
	basic.auth.user.info = [hidden]
	proxy.host = 
	schema.registry.basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2020-06-24 17:27:22.604  INFO 18164 --- [main] .c.s.b.k.s.KafkaStreamsFunctionProcessor : Key Serde used for processItem-in-0: io.confluent.kafka.streams.serdes.avro.PrimitiveAvroSerde
2020-06-24 17:27:22.605  INFO 18164 --- [main] i.c.k.s.KafkaAvroSerializerConfig        : KafkaAvroSerializerConfig values: 
	bearer.auth.token = [hidden]
	proxy.port = -1
	schema.reflection = false
	auto.register.schemas = true
	max.schemas.per.subject = 1000
	basic.auth.credentials.source = URL
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	schema.registry.url = [http://localhost:8081]
	basic.auth.user.info = [hidden]
	proxy.host = 
	schema.registry.basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2020-06-24 17:27:22.605  INFO 18164 --- [main] i.c.k.s.KafkaAvroDeserializerConfig      : KafkaAvroDeserializerConfig values: 
	bearer.auth.token = [hidden]
	proxy.port = -1
	schema.reflection = false
	auto.register.schemas = true
	max.schemas.per.subject = 1000
	basic.auth.credentials.source = URL
	specific.avro.reader = true
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	schema.registry.url = [http://localhost:8081]
	basic.auth.user.info = [hidden]
	proxy.host = 
	schema.registry.basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2020-06-24 17:27:22.605  INFO 18164 --- [main] .c.s.b.k.s.KafkaStreamsFunctionProcessor : Value Serde used for processItem-in-0: io.confluent.kafka.streams.serdes.avro.SpecificAvroSerde
2020-06-24 17:27:22.608  INFO 18164 --- [main] .k.s.AbstractKafkaStreamsBinderProcessor : Native decoding is enabled for processItem-in-0. Inbound deserialization done at the broker.
2020-06-24 17:27:22.616  INFO 18164 --- [main] .k.s.AbstractKafkaStreamsBinderProcessor : Binder Generated Kafka Streams Application ID: sinkInventory-applicationId
2020-06-24 17:27:22.616  INFO 18164 --- [main] .k.s.AbstractKafkaStreamsBinderProcessor : Use the binder generated application ID only for development and testing. 
2020-06-24 17:27:22.616  INFO 18164 --- [main] .k.s.AbstractKafkaStreamsBinderProcessor : For production deployments, please consider explicitly setting an application ID using a configuration property.
2020-06-24 17:27:22.616  INFO 18164 --- [main] .k.s.AbstractKafkaStreamsBinderProcessor : The generated applicationID is static and will be preserved over application restarts.
2020-06-24 17:27:22.624  INFO 18164 --- [main] i.c.k.s.KafkaAvroSerializerConfig        : KafkaAvroSerializerConfig values: 
	bearer.auth.token = [hidden]
	proxy.port = -1
	schema.reflection = false
	auto.register.schemas = true
	max.schemas.per.subject = 1000
	basic.auth.credentials.source = URL
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	schema.registry.url = [http://localhost:8081]
	basic.auth.user.info = [hidden]
	proxy.host = 
	schema.registry.basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2020-06-24 17:27:22.625  INFO 18164 --- [main] i.c.k.s.KafkaAvroDeserializerConfig      : KafkaAvroDeserializerConfig values: 
	bearer.auth.token = [hidden]
	proxy.port = -1
	schema.reflection = false
	auto.register.schemas = true
	max.schemas.per.subject = 1000
	basic.auth.credentials.source = URL
	specific.avro.reader = false
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	schema.registry.url = [http://localhost:8081]
	basic.auth.user.info = [hidden]
	proxy.host = 
	schema.registry.basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2020-06-24 17:27:22.625  INFO 18164 --- [main] .c.s.b.k.s.KafkaStreamsFunctionProcessor : Key Serde used for sinkInventory-in-0: io.confluent.kafka.streams.serdes.avro.PrimitiveAvroSerde
2020-06-24 17:27:22.625  INFO 18164 --- [main] i.c.k.s.KafkaAvroSerializerConfig        : KafkaAvroSerializerConfig values: 
	bearer.auth.token = [hidden]
	proxy.port = -1
	schema.reflection = false
	auto.register.schemas = true
	max.schemas.per.subject = 1000
	basic.auth.credentials.source = URL
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	schema.registry.url = [http://localhost:8081]
	basic.auth.user.info = [hidden]
	proxy.host = 
	schema.registry.basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2020-06-24 17:27:22.625  INFO 18164 --- [main] i.c.k.s.KafkaAvroDeserializerConfig      : KafkaAvroDeserializerConfig values: 
	bearer.auth.token = [hidden]
	proxy.port = -1
	schema.reflection = false
	auto.register.schemas = true
	max.schemas.per.subject = 1000
	basic.auth.credentials.source = URL
	specific.avro.reader = true
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	schema.registry.url = [http://localhost:8081]
	basic.auth.user.info = [hidden]
	proxy.host = 
	schema.registry.basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2020-06-24 17:27:22.626  INFO 18164 --- [main] .c.s.b.k.s.KafkaStreamsFunctionProcessor : Value Serde used for sinkInventory-in-0: io.confluent.kafka.streams.serdes.avro.SpecificAvroSerde
2020-06-24 17:27:22.626  INFO 18164 --- [main] .k.s.AbstractKafkaStreamsBinderProcessor : Native decoding is enabled for sinkInventory-in-0. Inbound deserialization done at the broker.
2020-06-24 17:27:22.634  INFO 18164 --- [main] .k.s.AbstractKafkaStreamsBinderProcessor : Binder Generated Kafka Streams Application ID: sinkItem-applicationId
2020-06-24 17:27:22.635  INFO 18164 --- [main] .k.s.AbstractKafkaStreamsBinderProcessor : Use the binder generated application ID only for development and testing. 
2020-06-24 17:27:22.635  INFO 18164 --- [main] .k.s.AbstractKafkaStreamsBinderProcessor : For production deployments, please consider explicitly setting an application ID using a configuration property.
2020-06-24 17:27:22.635  INFO 18164 --- [main] .k.s.AbstractKafkaStreamsBinderProcessor : The generated applicationID is static and will be preserved over application restarts.
2020-06-24 17:27:22.643  INFO 18164 --- [main] i.c.k.s.KafkaAvroSerializerConfig        : KafkaAvroSerializerConfig values: 
	bearer.auth.token = [hidden]
	proxy.port = -1
	schema.reflection = false
	auto.register.schemas = true
	max.schemas.per.subject = 1000
	basic.auth.credentials.source = URL
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	schema.registry.url = [http://localhost:8081]
	basic.auth.user.info = [hidden]
	proxy.host = 
	schema.registry.basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2020-06-24 17:27:22.644  INFO 18164 --- [main] i.c.k.s.KafkaAvroDeserializerConfig      : KafkaAvroDeserializerConfig values: 
	bearer.auth.token = [hidden]
	proxy.port = -1
	schema.reflection = false
	auto.register.schemas = true
	max.schemas.per.subject = 1000
	basic.auth.credentials.source = URL
	specific.avro.reader = false
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	schema.registry.url = [http://localhost:8081]
	basic.auth.user.info = [hidden]
	proxy.host = 
	schema.registry.basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2020-06-24 17:27:22.644  INFO 18164 --- [main] .c.s.b.k.s.KafkaStreamsFunctionProcessor : Key Serde used for sinkItem-in-0: io.confluent.kafka.streams.serdes.avro.PrimitiveAvroSerde
2020-06-24 17:27:22.644  INFO 18164 --- [main] i.c.k.s.KafkaAvroSerializerConfig        : KafkaAvroSerializerConfig values: 
	bearer.auth.token = [hidden]
	proxy.port = -1
	schema.reflection = false
	auto.register.schemas = true
	max.schemas.per.subject = 1000
	basic.auth.credentials.source = URL
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	schema.registry.url = [http://localhost:8081]
	basic.auth.user.info = [hidden]
	proxy.host = 
	schema.registry.basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2020-06-24 17:27:22.644  INFO 18164 --- [main] i.c.k.s.KafkaAvroDeserializerConfig      : KafkaAvroDeserializerConfig values: 
	bearer.auth.token = [hidden]
	proxy.port = -1
	schema.reflection = false
	auto.register.schemas = true
	max.schemas.per.subject = 1000
	basic.auth.credentials.source = URL
	specific.avro.reader = true
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	schema.registry.url = [http://localhost:8081]
	basic.auth.user.info = [hidden]
	proxy.host = 
	schema.registry.basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2020-06-24 17:27:22.644  INFO 18164 --- [main] .c.s.b.k.s.KafkaStreamsFunctionProcessor : Value Serde used for sinkItem-in-0: io.confluent.kafka.streams.serdes.avro.SpecificAvroSerde
2020-06-24 17:27:22.645  INFO 18164 --- [main] .k.s.AbstractKafkaStreamsBinderProcessor : Native decoding is enabled for sinkItem-in-0. Inbound deserialization done at the broker.
2020-06-24 17:27:22.765  INFO 18164 --- [main] o.s.i.monitor.IntegrationMBeanExporter   : Registering MessageChannel errorChannel
2020-06-24 17:27:22.851  INFO 18164 --- [main] o.s.i.monitor.IntegrationMBeanExporter   : Registering MessageChannel output
2020-06-24 17:27:22.880  INFO 18164 --- [main] o.s.i.monitor.IntegrationMBeanExporter   : Registering MessageChannel nullChannel
2020-06-24 17:27:22.897  INFO 18164 --- [main] o.s.i.monitor.IntegrationMBeanExporter   : Registering MessageHandler errorLogger
2020-06-24 17:27:22.921  INFO 18164 --- [main] o.s.i.endpoint.EventDrivenConsumer       : Adding {logging-channel-adapter:_org.springframework.integration.errorLogger} as a subscriber to the 'errorChannel' channel
2020-06-24 17:27:22.921  INFO 18164 --- [main] o.s.i.channel.PublishSubscribeChannel    : Channel 'application.errorChannel' has 1 subscriber(s).
2020-06-24 17:27:22.921  INFO 18164 --- [main] o.s.i.endpoint.EventDrivenConsumer       : started bean '_org.springframework.integration.errorLogger'
2020-06-24 17:27:22.922  INFO 18164 --- [main] o.s.c.s.binder.DefaultBinderFactory      : Creating binder: ktable
2020-06-24 17:27:22.929 DEBUG 18164 --- [main] o.s.boot.env.OriginTrackedYamlLoader     : Loading from YAML: class path resource [application.yml]
2020-06-24 17:27:22.935 DEBUG 18164 --- [main] o.s.boot.env.OriginTrackedYamlLoader     : Merging document (no matchers set): {debug=true, logging={file={name=app.log}, level={root=info, org.apache.kafka.streams=info}}, server={port=9000}, spring={cloud={stream={function={definition=sinkItem, sinkInventory, processItem}, bindings={sinkItem-in-0={destination=items, group=item-readers}, sinkInventory-in-0={destination=inventory, group=inventory-readers}, processItem-in-0={destination=items}, processItem-out-0={destination=inventory}, output={content-type=application/*+avro, producer={use-native-encoding=true}}}, kafka={binder={auto-create-topics=false, autoAddPartitions=false, brokers=[localhost:19093, localhost:19094], configuration={specific.avro.reader=true}, producer-properties={key.serializer=io.confluent.kafka.serializers.KafkaAvroSerializer, value.serializer=io.confluent.kafka.serializers.KafkaAvroSerializer, schema.registry.url=http://localhost:8081}}, streams={default={consumer={keySerde=io.confluent.kafka.streams.serdes.avro.PrimitiveAvroSerde, valueSerde=io.confluent.kafka.streams.serdes.avro.SpecificAvroSerde}, producer={keySerde=io.confluent.kafka.streams.serdes.avro.PrimitiveAvroSerde, valueSerde=io.confluent.kafka.streams.serdes.avro.SpecificAvroSerde}}, binder={brokers=[localhost:19093, localhost:19094], configuration={schema.registry.url=http://localhost:8081, default={key.serde=io.confluent.kafka.streams.serdes.avro.PrimitiveAvroSerde, value.serde=io.confluent.kafka.streams.serdes.avro.SpecificAvroSerde}}, auto-create-topics=false, autoAddPartitions=false}}}}}}}
2020-06-24 17:27:22.935 DEBUG 18164 --- [main] o.s.boot.env.OriginTrackedYamlLoader     : Loaded 1 document from YAML resource: class path resource [application.yml]
2020-06-24 17:27:22.950 DEBUG 18164 --- [main] .c.l.ClasspathLoggingApplicationListener : Application started with classpath: unknown
2020-06-24 17:27:22.956 DEBUG 18164 --- [main] o.s.boot.SpringApplication               : Loading source class org.springframework.cloud.stream.binder.kafka.streams.KTableBinderConfiguration
2020-06-24 17:27:22.958 DEBUG 18164 --- [main] o.s.b.c.c.ConfigFileApplicationListener  : Loaded config file 'file:/Q:/dev/projects/inhouse/event-driven-kafka/shop/services/inventory/target/classes/application.yml' (classpath:/application.yml)
2020-06-24 17:27:23.009 DEBUG 18164 --- [main] ConditionEvaluationReportLoggingListener : 


============================
CONDITIONS EVALUATION REPORT
============================


Positive matches:
-----------------

   KafkaAnnotationDrivenConfiguration matched:
      - @ConditionalOnClass found required class 'org.springframework.kafka.annotation.EnableKafka' (OnClassCondition)

   KafkaAutoConfiguration matched:
      - @ConditionalOnClass found required class 'org.springframework.kafka.core.KafkaTemplate' (OnClassCondition)


Negative matches:
-----------------

   KTableBinderConfiguration#outerContextBeanFactoryPostProcessor:
      Did not match:
         - @ConditionalOnBean (names: outerContext; SearchStrategy: all) did not find any beans named outerContext (OnBeanCondition)

   KafkaAnnotationDrivenConfiguration#kafkaListenerContainerFactory:
      Did not match:
         - @ConditionalOnMissingBean (names: kafkaListenerContainerFactory; SearchStrategy: all) found beans named kafkaListenerContainerFactory (OnBeanCondition)

   KafkaAnnotationDrivenConfiguration#kafkaListenerContainerFactoryConfigurer:
      Did not match:
         - @ConditionalOnMissingBean (types: org.springframework.boot.autoconfigure.kafka.ConcurrentKafkaListenerContainerFactoryConfigurer; SearchStrategy: all) found beans of type 'org.springframework.boot.autoconfigure.kafka.ConcurrentKafkaListenerContainerFactoryConfigurer' kafkaListenerContainerFactoryConfigurer (OnBeanCondition)

   KafkaAnnotationDrivenConfiguration.EnableKafkaConfiguration:
      Did not match:
         - @ConditionalOnMissingBean (names: org.springframework.kafka.config.internalKafkaListenerAnnotationProcessor; SearchStrategy: all) found beans named org.springframework.kafka.config.internalKafkaListenerAnnotationProcessor (OnBeanCondition)

   KafkaAutoConfiguration#kafkaAdmin:
      Did not match:
         - @ConditionalOnMissingBean (types: org.springframework.kafka.core.KafkaAdmin; SearchStrategy: all) found beans of type 'org.springframework.kafka.core.KafkaAdmin' kafkaAdmin (OnBeanCondition)

   KafkaAutoConfiguration#kafkaConsumerFactory:
      Did not match:
         - @ConditionalOnMissingBean (types: org.springframework.kafka.core.ConsumerFactory; SearchStrategy: all) found beans of type 'org.springframework.kafka.core.ConsumerFactory' kafkaConsumerFactory (OnBeanCondition)

   KafkaAutoConfiguration#kafkaJaasInitializer:
      Did not match:
         - @ConditionalOnProperty (spring.kafka.jaas.enabled) did not find property 'spring.kafka.jaas.enabled' (OnPropertyCondition)

   KafkaAutoConfiguration#kafkaProducerFactory:
      Did not match:
         - @ConditionalOnMissingBean (types: org.springframework.kafka.core.ProducerFactory; SearchStrategy: all) found beans of type 'org.springframework.kafka.core.ProducerFactory' kafkaProducerFactory (OnBeanCondition)

   KafkaAutoConfiguration#kafkaProducerListener:
      Did not match:
         - @ConditionalOnMissingBean (types: org.springframework.kafka.support.ProducerListener; SearchStrategy: all) found beans of type 'org.springframework.kafka.support.ProducerListener' kafkaProducerListener (OnBeanCondition)

   KafkaAutoConfiguration#kafkaTemplate:
      Did not match:
         - @ConditionalOnMissingBean (types: org.springframework.kafka.core.KafkaTemplate; SearchStrategy: all) found beans of type 'org.springframework.kafka.core.KafkaTemplate' kafkaTemplate (OnBeanCondition)

   KafkaAutoConfiguration#kafkaTransactionManager:
      Did not match:
         - @ConditionalOnProperty (spring.kafka.producer.transaction-id-prefix) did not find property 'spring.kafka.producer.transaction-id-prefix' (OnPropertyCondition)

   KafkaStreamsAnnotationDrivenConfiguration:
      Did not match:
         - @ConditionalOnBean (names: defaultKafkaStreamsBuilder; SearchStrategy: all) did not find any beans named defaultKafkaStreamsBuilder (OnBeanCondition)
      Matched:
         - @ConditionalOnClass found required class 'org.apache.kafka.streams.StreamsBuilder' (OnClassCondition)

   KafkaStreamsBinderHealthIndicatorConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'org.springframework.boot.actuate.health.HealthIndicator' (OnClassCondition)

   MultiBinderPropertiesConfiguration#kafkaBinderConfigurationProperties:
      Did not match:
         - @ConditionalOnBean (names: outerContext; SearchStrategy: all) did not find any beans named outerContext (OnBeanCondition)


Exclusions:
-----------

    None


Unconditional classes:
----------------------

    None



2020-06-24 17:27:23.013  INFO 18164 --- [main] o.s.c.s.binder.DefaultBinderFactory      : Caching the binder: ktable
2020-06-24 17:27:23.013  INFO 18164 --- [main] o.s.c.s.binder.DefaultBinderFactory      : Retrieving cached binder: ktable
2020-06-24 17:27:23.014  INFO 18164 --- [main] o.s.c.s.binder.DefaultBinderFactory      : Creating binder: kafka
2020-06-24 17:27:23.019 DEBUG 18164 --- [main] o.s.boot.env.OriginTrackedYamlLoader     : Loading from YAML: class path resource [application.yml]
2020-06-24 17:27:23.023 DEBUG 18164 --- [main] o.s.boot.env.OriginTrackedYamlLoader     : Merging document (no matchers set): {debug=true, logging={file={name=app.log}, level={root=info, org.apache.kafka.streams=info}}, server={port=9000}, spring={cloud={stream={function={definition=sinkItem, sinkInventory, processItem}, bindings={sinkItem-in-0={destination=items, group=item-readers}, sinkInventory-in-0={destination=inventory, group=inventory-readers}, processItem-in-0={destination=items}, processItem-out-0={destination=inventory}, output={content-type=application/*+avro, producer={use-native-encoding=true}}}, kafka={binder={auto-create-topics=false, autoAddPartitions=false, brokers=[localhost:19093, localhost:19094], configuration={specific.avro.reader=true}, producer-properties={key.serializer=io.confluent.kafka.serializers.KafkaAvroSerializer, value.serializer=io.confluent.kafka.serializers.KafkaAvroSerializer, schema.registry.url=http://localhost:8081}}, streams={default={consumer={keySerde=io.confluent.kafka.streams.serdes.avro.PrimitiveAvroSerde, valueSerde=io.confluent.kafka.streams.serdes.avro.SpecificAvroSerde}, producer={keySerde=io.confluent.kafka.streams.serdes.avro.PrimitiveAvroSerde, valueSerde=io.confluent.kafka.streams.serdes.avro.SpecificAvroSerde}}, binder={brokers=[localhost:19093, localhost:19094], configuration={schema.registry.url=http://localhost:8081, default={key.serde=io.confluent.kafka.streams.serdes.avro.PrimitiveAvroSerde, value.serde=io.confluent.kafka.streams.serdes.avro.SpecificAvroSerde}}, auto-create-topics=false, autoAddPartitions=false}}}}}}}
2020-06-24 17:27:23.023 DEBUG 18164 --- [main] o.s.boot.env.OriginTrackedYamlLoader     : Loaded 1 document from YAML resource: class path resource [application.yml]
2020-06-24 17:27:23.034 DEBUG 18164 --- [main] .c.l.ClasspathLoggingApplicationListener : Application started with classpath: unknown
2020-06-24 17:27:23.037 DEBUG 18164 --- [main] o.s.boot.SpringApplication               : Loading source class org.springframework.cloud.stream.binder.kafka.config.KafkaBinderConfiguration
2020-06-24 17:27:23.040 DEBUG 18164 --- [main] o.s.b.c.c.ConfigFileApplicationListener  : Loaded config file 'file:/Q:/dev/projects/inhouse/event-driven-kafka/shop/services/inventory/target/classes/application.yml' (classpath:/application.yml)
2020-06-24 17:27:23.088 DEBUG 18164 --- [main] ConditionEvaluationReportLoggingListener : 


============================
CONDITIONS EVALUATION REPORT
============================


Positive matches:
-----------------

   KafkaAnnotationDrivenConfiguration matched:
      - @ConditionalOnClass found required class 'org.springframework.kafka.annotation.EnableKafka' (OnClassCondition)

   KafkaAutoConfiguration matched:
      - @ConditionalOnClass found required class 'org.springframework.kafka.core.KafkaTemplate' (OnClassCondition)

   KafkaBinderConfiguration matched:
      - @ConditionalOnMissingBean (types: org.springframework.cloud.stream.binder.Binder; SearchStrategy: all) did not find any beans (OnBeanCondition)

   KafkaBinderConfiguration#jaasInitializer matched:
      - @ConditionalOnMissingBean (types: org.springframework.kafka.security.jaas.KafkaJaasLoginModuleInitializer; SearchStrategy: all) did not find any beans (OnBeanCondition)

   KafkaBinderConfiguration#kafkaNullConverter matched:
      - @ConditionalOnMissingBean (types: org.springframework.cloud.stream.binder.kafka.KafkaNullConverter; SearchStrategy: all) did not find any beans (OnBeanCondition)


Negative matches:
-----------------

   KafkaAnnotationDrivenConfiguration#kafkaListenerContainerFactory:
      Did not match:
         - @ConditionalOnMissingBean (names: kafkaListenerContainerFactory; SearchStrategy: all) found beans named kafkaListenerContainerFactory (OnBeanCondition)

   KafkaAnnotationDrivenConfiguration#kafkaListenerContainerFactoryConfigurer:
      Did not match:
         - @ConditionalOnMissingBean (types: org.springframework.boot.autoconfigure.kafka.ConcurrentKafkaListenerContainerFactoryConfigurer; SearchStrategy: all) found beans of type 'org.springframework.boot.autoconfigure.kafka.ConcurrentKafkaListenerContainerFactoryConfigurer' kafkaListenerContainerFactoryConfigurer (OnBeanCondition)

   KafkaAnnotationDrivenConfiguration.EnableKafkaConfiguration:
      Did not match:
         - @ConditionalOnMissingBean (names: org.springframework.kafka.config.internalKafkaListenerAnnotationProcessor; SearchStrategy: all) found beans named org.springframework.kafka.config.internalKafkaListenerAnnotationProcessor (OnBeanCondition)

   KafkaAutoConfiguration#kafkaAdmin:
      Did not match:
         - @ConditionalOnMissingBean (types: org.springframework.kafka.core.KafkaAdmin; SearchStrategy: all) found beans of type 'org.springframework.kafka.core.KafkaAdmin' kafkaAdmin (OnBeanCondition)

   KafkaAutoConfiguration#kafkaConsumerFactory:
      Did not match:
         - @ConditionalOnMissingBean (types: org.springframework.kafka.core.ConsumerFactory; SearchStrategy: all) found beans of type 'org.springframework.kafka.core.ConsumerFactory' kafkaConsumerFactory (OnBeanCondition)

   KafkaAutoConfiguration#kafkaJaasInitializer:
      Did not match:
         - @ConditionalOnProperty (spring.kafka.jaas.enabled) did not find property 'spring.kafka.jaas.enabled' (OnPropertyCondition)

   KafkaAutoConfiguration#kafkaProducerFactory:
      Did not match:
         - @ConditionalOnMissingBean (types: org.springframework.kafka.core.ProducerFactory; SearchStrategy: all) found beans of type 'org.springframework.kafka.core.ProducerFactory' kafkaProducerFactory (OnBeanCondition)

   KafkaAutoConfiguration#kafkaProducerListener:
      Did not match:
         - @ConditionalOnMissingBean (types: org.springframework.kafka.support.ProducerListener; SearchStrategy: all) found beans of type 'org.springframework.kafka.support.ProducerListener' kafkaProducerListener (OnBeanCondition)

   KafkaAutoConfiguration#kafkaTemplate:
      Did not match:
         - @ConditionalOnMissingBean (types: org.springframework.kafka.core.KafkaTemplate; SearchStrategy: all) found beans of type 'org.springframework.kafka.core.KafkaTemplate' kafkaTemplate (OnBeanCondition)

   KafkaAutoConfiguration#kafkaTransactionManager:
      Did not match:
         - @ConditionalOnProperty (spring.kafka.producer.transaction-id-prefix) did not find property 'spring.kafka.producer.transaction-id-prefix' (OnPropertyCondition)

   KafkaBinderConfiguration#producerListener:
      Did not match:
         - @ConditionalOnMissingBean (types: org.springframework.kafka.support.ProducerListener; SearchStrategy: all) found beans of type 'org.springframework.kafka.support.ProducerListener' kafkaProducerListener (OnBeanCondition)

   KafkaBinderConfiguration.KafkaBinderMetricsConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'io.micrometer.core.instrument.MeterRegistry' (OnClassCondition)

   KafkaBinderConfiguration.KafkaBinderMetricsConfigurationWithMultiBinder:
      Did not match:
         - @ConditionalOnClass did not find required class 'io.micrometer.core.instrument.MeterRegistry' (OnClassCondition)

   KafkaBinderHealthIndicatorConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'org.springframework.boot.actuate.health.HealthIndicator' (OnClassCondition)

   KafkaStreamsAnnotationDrivenConfiguration:
      Did not match:
         - @ConditionalOnBean (names: defaultKafkaStreamsBuilder; SearchStrategy: all) did not find any beans named defaultKafkaStreamsBuilder (OnBeanCondition)
      Matched:
         - @ConditionalOnClass found required class 'org.apache.kafka.streams.StreamsBuilder' (OnClassCondition)


Exclusions:
-----------

    None


Unconditional classes:
----------------------

    None



2020-06-24 17:27:23.089  INFO 18164 --- [main] o.s.c.s.binder.DefaultBinderFactory      : Caching the binder: kafka
2020-06-24 17:27:23.089  INFO 18164 --- [main] o.s.c.s.binder.DefaultBinderFactory      : Retrieving cached binder: kafka
2020-06-24 17:27:23.089  INFO 18164 --- [main] o.s.c.s.binder.DefaultBinderFactory      : Creating binder: kstream
2020-06-24 17:27:23.094 DEBUG 18164 --- [main] o.s.boot.env.OriginTrackedYamlLoader     : Loading from YAML: class path resource [application.yml]
2020-06-24 17:27:23.097 DEBUG 18164 --- [main] o.s.boot.env.OriginTrackedYamlLoader     : Merging document (no matchers set): {debug=true, logging={file={name=app.log}, level={root=info, org.apache.kafka.streams=info}}, server={port=9000}, spring={cloud={stream={function={definition=sinkItem, sinkInventory, processItem}, bindings={sinkItem-in-0={destination=items, group=item-readers}, sinkInventory-in-0={destination=inventory, group=inventory-readers}, processItem-in-0={destination=items}, processItem-out-0={destination=inventory}, output={content-type=application/*+avro, producer={use-native-encoding=true}}}, kafka={binder={auto-create-topics=false, autoAddPartitions=false, brokers=[localhost:19093, localhost:19094], configuration={specific.avro.reader=true}, producer-properties={key.serializer=io.confluent.kafka.serializers.KafkaAvroSerializer, value.serializer=io.confluent.kafka.serializers.KafkaAvroSerializer, schema.registry.url=http://localhost:8081}}, streams={default={consumer={keySerde=io.confluent.kafka.streams.serdes.avro.PrimitiveAvroSerde, valueSerde=io.confluent.kafka.streams.serdes.avro.SpecificAvroSerde}, producer={keySerde=io.confluent.kafka.streams.serdes.avro.PrimitiveAvroSerde, valueSerde=io.confluent.kafka.streams.serdes.avro.SpecificAvroSerde}}, binder={brokers=[localhost:19093, localhost:19094], configuration={schema.registry.url=http://localhost:8081, default={key.serde=io.confluent.kafka.streams.serdes.avro.PrimitiveAvroSerde, value.serde=io.confluent.kafka.streams.serdes.avro.SpecificAvroSerde}}, auto-create-topics=false, autoAddPartitions=false}}}}}}}
2020-06-24 17:27:23.097 DEBUG 18164 --- [main] o.s.boot.env.OriginTrackedYamlLoader     : Loaded 1 document from YAML resource: class path resource [application.yml]
2020-06-24 17:27:23.112 DEBUG 18164 --- [main] .c.l.ClasspathLoggingApplicationListener : Application started with classpath: unknown
2020-06-24 17:27:23.116 DEBUG 18164 --- [main] o.s.boot.SpringApplication               : Loading source class org.springframework.cloud.stream.binder.kafka.streams.KStreamBinderConfiguration
2020-06-24 17:27:23.117 DEBUG 18164 --- [main] o.s.b.c.c.ConfigFileApplicationListener  : Loaded config file 'file:/Q:/dev/projects/inhouse/event-driven-kafka/shop/services/inventory/target/classes/application.yml' (classpath:/application.yml)
2020-06-24 17:27:23.137 DEBUG 18164 --- [main] ConditionEvaluationReportLoggingListener : 


============================
CONDITIONS EVALUATION REPORT
============================


Positive matches:
-----------------

   KafkaAnnotationDrivenConfiguration matched:
      - @ConditionalOnClass found required class 'org.springframework.kafka.annotation.EnableKafka' (OnClassCondition)

   KafkaAutoConfiguration matched:
      - @ConditionalOnClass found required class 'org.springframework.kafka.core.KafkaTemplate' (OnClassCondition)


Negative matches:
-----------------

   KStreamBinderConfiguration#outerContextBeanFactoryPostProcessor:
      Did not match:
         - @ConditionalOnBean (names: outerContext; SearchStrategy: all) did not find any beans named outerContext (OnBeanCondition)

   KafkaAnnotationDrivenConfiguration#kafkaListenerContainerFactory:
      Did not match:
         - @ConditionalOnMissingBean (names: kafkaListenerContainerFactory; SearchStrategy: all) found beans named kafkaListenerContainerFactory (OnBeanCondition)

   KafkaAnnotationDrivenConfiguration#kafkaListenerContainerFactoryConfigurer:
      Did not match:
         - @ConditionalOnMissingBean (types: org.springframework.boot.autoconfigure.kafka.ConcurrentKafkaListenerContainerFactoryConfigurer; SearchStrategy: all) found beans of type 'org.springframework.boot.autoconfigure.kafka.ConcurrentKafkaListenerContainerFactoryConfigurer' kafkaListenerContainerFactoryConfigurer (OnBeanCondition)

   KafkaAnnotationDrivenConfiguration.EnableKafkaConfiguration:
      Did not match:
         - @ConditionalOnMissingBean (names: org.springframework.kafka.config.internalKafkaListenerAnnotationProcessor; SearchStrategy: all) found beans named org.springframework.kafka.config.internalKafkaListenerAnnotationProcessor (OnBeanCondition)

   KafkaAutoConfiguration#kafkaAdmin:
      Did not match:
         - @ConditionalOnMissingBean (types: org.springframework.kafka.core.KafkaAdmin; SearchStrategy: all) found beans of type 'org.springframework.kafka.core.KafkaAdmin' kafkaAdmin (OnBeanCondition)

   KafkaAutoConfiguration#kafkaConsumerFactory:
      Did not match:
         - @ConditionalOnMissingBean (types: org.springframework.kafka.core.ConsumerFactory; SearchStrategy: all) found beans of type 'org.springframework.kafka.core.ConsumerFactory' kafkaConsumerFactory (OnBeanCondition)

   KafkaAutoConfiguration#kafkaJaasInitializer:
      Did not match:
         - @ConditionalOnProperty (spring.kafka.jaas.enabled) did not find property 'spring.kafka.jaas.enabled' (OnPropertyCondition)

   KafkaAutoConfiguration#kafkaProducerFactory:
      Did not match:
         - @ConditionalOnMissingBean (types: org.springframework.kafka.core.ProducerFactory; SearchStrategy: all) found beans of type 'org.springframework.kafka.core.ProducerFactory' kafkaProducerFactory (OnBeanCondition)

   KafkaAutoConfiguration#kafkaProducerListener:
      Did not match:
         - @ConditionalOnMissingBean (types: org.springframework.kafka.support.ProducerListener; SearchStrategy: all) found beans of type 'org.springframework.kafka.support.ProducerListener' kafkaProducerListener (OnBeanCondition)

   KafkaAutoConfiguration#kafkaTemplate:
      Did not match:
         - @ConditionalOnMissingBean (types: org.springframework.kafka.core.KafkaTemplate; SearchStrategy: all) found beans of type 'org.springframework.kafka.core.KafkaTemplate' kafkaTemplate (OnBeanCondition)

   KafkaAutoConfiguration#kafkaTransactionManager:
      Did not match:
         - @ConditionalOnProperty (spring.kafka.producer.transaction-id-prefix) did not find property 'spring.kafka.producer.transaction-id-prefix' (OnPropertyCondition)

   KafkaStreamsAnnotationDrivenConfiguration:
      Did not match:
         - @ConditionalOnBean (names: defaultKafkaStreamsBuilder; SearchStrategy: all) did not find any beans named defaultKafkaStreamsBuilder (OnBeanCondition)
      Matched:
         - @ConditionalOnClass found required class 'org.apache.kafka.streams.StreamsBuilder' (OnClassCondition)

   KafkaStreamsBinderHealthIndicatorConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'org.springframework.boot.actuate.health.HealthIndicator' (OnClassCondition)

   MultiBinderPropertiesConfiguration#kafkaBinderConfigurationProperties:
      Did not match:
         - @ConditionalOnBean (names: outerContext; SearchStrategy: all) did not find any beans named outerContext (OnBeanCondition)


Exclusions:
-----------

    None


Unconditional classes:
----------------------

    None



2020-06-24 17:27:23.138  INFO 18164 --- [main] o.s.c.s.binder.DefaultBinderFactory      : Caching the binder: kstream
2020-06-24 17:27:23.138  INFO 18164 --- [main] o.s.c.s.binder.DefaultBinderFactory      : Retrieving cached binder: kstream
2020-06-24 17:27:23.138  INFO 18164 --- [main] o.s.c.s.binder.DefaultBinderFactory      : Creating binder: globalktable
2020-06-24 17:27:23.144 DEBUG 18164 --- [main] o.s.boot.env.OriginTrackedYamlLoader     : Loading from YAML: class path resource [application.yml]
2020-06-24 17:27:23.146 DEBUG 18164 --- [main] o.s.boot.env.OriginTrackedYamlLoader     : Merging document (no matchers set): {debug=true, logging={file={name=app.log}, level={root=info, org.apache.kafka.streams=info}}, server={port=9000}, spring={cloud={stream={function={definition=sinkItem, sinkInventory, processItem}, bindings={sinkItem-in-0={destination=items, group=item-readers}, sinkInventory-in-0={destination=inventory, group=inventory-readers}, processItem-in-0={destination=items}, processItem-out-0={destination=inventory}, output={content-type=application/*+avro, producer={use-native-encoding=true}}}, kafka={binder={auto-create-topics=false, autoAddPartitions=false, brokers=[localhost:19093, localhost:19094], configuration={specific.avro.reader=true}, producer-properties={key.serializer=io.confluent.kafka.serializers.KafkaAvroSerializer, value.serializer=io.confluent.kafka.serializers.KafkaAvroSerializer, schema.registry.url=http://localhost:8081}}, streams={default={consumer={keySerde=io.confluent.kafka.streams.serdes.avro.PrimitiveAvroSerde, valueSerde=io.confluent.kafka.streams.serdes.avro.SpecificAvroSerde}, producer={keySerde=io.confluent.kafka.streams.serdes.avro.PrimitiveAvroSerde, valueSerde=io.confluent.kafka.streams.serdes.avro.SpecificAvroSerde}}, binder={brokers=[localhost:19093, localhost:19094], configuration={schema.registry.url=http://localhost:8081, default={key.serde=io.confluent.kafka.streams.serdes.avro.PrimitiveAvroSerde, value.serde=io.confluent.kafka.streams.serdes.avro.SpecificAvroSerde}}, auto-create-topics=false, autoAddPartitions=false}}}}}}}
2020-06-24 17:27:23.146 DEBUG 18164 --- [main] o.s.boot.env.OriginTrackedYamlLoader     : Loaded 1 document from YAML resource: class path resource [application.yml]
2020-06-24 17:27:23.158 DEBUG 18164 --- [main] .c.l.ClasspathLoggingApplicationListener : Application started with classpath: unknown
2020-06-24 17:27:23.161 DEBUG 18164 --- [main] o.s.boot.SpringApplication               : Loading source class org.springframework.cloud.stream.binder.kafka.streams.GlobalKTableBinderConfiguration
2020-06-24 17:27:23.163 DEBUG 18164 --- [main] o.s.b.c.c.ConfigFileApplicationListener  : Loaded config file 'file:/Q:/dev/projects/inhouse/event-driven-kafka/shop/services/inventory/target/classes/application.yml' (classpath:/application.yml)
2020-06-24 17:27:23.187 DEBUG 18164 --- [main] ConditionEvaluationReportLoggingListener : 


============================
CONDITIONS EVALUATION REPORT
============================


Positive matches:
-----------------

   KafkaAnnotationDrivenConfiguration matched:
      - @ConditionalOnClass found required class 'org.springframework.kafka.annotation.EnableKafka' (OnClassCondition)

   KafkaAutoConfiguration matched:
      - @ConditionalOnClass found required class 'org.springframework.kafka.core.KafkaTemplate' (OnClassCondition)


Negative matches:
-----------------

   GlobalKTableBinderConfiguration#outerContextBeanFactoryPostProcessor:
      Did not match:
         - @ConditionalOnBean (names: outerContext; SearchStrategy: all) did not find any beans named outerContext (OnBeanCondition)

   KafkaAnnotationDrivenConfiguration#kafkaListenerContainerFactory:
      Did not match:
         - @ConditionalOnMissingBean (names: kafkaListenerContainerFactory; SearchStrategy: all) found beans named kafkaListenerContainerFactory (OnBeanCondition)

   KafkaAnnotationDrivenConfiguration#kafkaListenerContainerFactoryConfigurer:
      Did not match:
         - @ConditionalOnMissingBean (types: org.springframework.boot.autoconfigure.kafka.ConcurrentKafkaListenerContainerFactoryConfigurer; SearchStrategy: all) found beans of type 'org.springframework.boot.autoconfigure.kafka.ConcurrentKafkaListenerContainerFactoryConfigurer' kafkaListenerContainerFactoryConfigurer (OnBeanCondition)

   KafkaAnnotationDrivenConfiguration.EnableKafkaConfiguration:
      Did not match:
         - @ConditionalOnMissingBean (names: org.springframework.kafka.config.internalKafkaListenerAnnotationProcessor; SearchStrategy: all) found beans named org.springframework.kafka.config.internalKafkaListenerAnnotationProcessor (OnBeanCondition)

   KafkaAutoConfiguration#kafkaAdmin:
      Did not match:
         - @ConditionalOnMissingBean (types: org.springframework.kafka.core.KafkaAdmin; SearchStrategy: all) found beans of type 'org.springframework.kafka.core.KafkaAdmin' kafkaAdmin (OnBeanCondition)

   KafkaAutoConfiguration#kafkaConsumerFactory:
      Did not match:
         - @ConditionalOnMissingBean (types: org.springframework.kafka.core.ConsumerFactory; SearchStrategy: all) found beans of type 'org.springframework.kafka.core.ConsumerFactory' kafkaConsumerFactory (OnBeanCondition)

   KafkaAutoConfiguration#kafkaJaasInitializer:
      Did not match:
         - @ConditionalOnProperty (spring.kafka.jaas.enabled) did not find property 'spring.kafka.jaas.enabled' (OnPropertyCondition)

   KafkaAutoConfiguration#kafkaProducerFactory:
      Did not match:
         - @ConditionalOnMissingBean (types: org.springframework.kafka.core.ProducerFactory; SearchStrategy: all) found beans of type 'org.springframework.kafka.core.ProducerFactory' kafkaProducerFactory (OnBeanCondition)

   KafkaAutoConfiguration#kafkaProducerListener:
      Did not match:
         - @ConditionalOnMissingBean (types: org.springframework.kafka.support.ProducerListener; SearchStrategy: all) found beans of type 'org.springframework.kafka.support.ProducerListener' kafkaProducerListener (OnBeanCondition)

   KafkaAutoConfiguration#kafkaTemplate:
      Did not match:
         - @ConditionalOnMissingBean (types: org.springframework.kafka.core.KafkaTemplate; SearchStrategy: all) found beans of type 'org.springframework.kafka.core.KafkaTemplate' kafkaTemplate (OnBeanCondition)

   KafkaAutoConfiguration#kafkaTransactionManager:
      Did not match:
         - @ConditionalOnProperty (spring.kafka.producer.transaction-id-prefix) did not find property 'spring.kafka.producer.transaction-id-prefix' (OnPropertyCondition)

   KafkaStreamsAnnotationDrivenConfiguration:
      Did not match:
         - @ConditionalOnBean (names: defaultKafkaStreamsBuilder; SearchStrategy: all) did not find any beans named defaultKafkaStreamsBuilder (OnBeanCondition)
      Matched:
         - @ConditionalOnClass found required class 'org.apache.kafka.streams.StreamsBuilder' (OnClassCondition)

   KafkaStreamsBinderHealthIndicatorConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'org.springframework.boot.actuate.health.HealthIndicator' (OnClassCondition)

   MultiBinderPropertiesConfiguration#kafkaBinderConfigurationProperties:
      Did not match:
         - @ConditionalOnBean (names: outerContext; SearchStrategy: all) did not find any beans named outerContext (OnBeanCondition)


Exclusions:
-----------

    None


Unconditional classes:
----------------------

    None



2020-06-24 17:27:23.188  INFO 18164 --- [main] o.s.c.s.binder.DefaultBinderFactory      : Caching the binder: globalktable
2020-06-24 17:27:23.188  INFO 18164 --- [main] o.s.c.s.binder.DefaultBinderFactory      : Retrieving cached binder: globalktable
2020-06-24 17:27:23.188  INFO 18164 --- [main] o.s.c.s.binder.DefaultBinderFactory      : Retrieving cached binder: kafka
2020-06-24 17:27:23.236  INFO 18164 --- [main] o.s.c.s.b.k.p.KafkaTopicProvisioner      : Using kafka topic for outbound: output
2020-06-24 17:27:23.238  INFO 18164 --- [main] o.a.k.clients.admin.AdminClientConfig    : AdminClientConfig values: 
	bootstrap.servers = [localhost:19093, localhost:19094]
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2020-06-24 17:27:23.525  INFO 18164 --- [main] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 2.5.0
2020-06-24 17:27:23.526  INFO 18164 --- [main] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: 66563e712b0b9f84
2020-06-24 17:27:23.526  INFO 18164 --- [main] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1593008843523
2020-06-24 17:27:23.529  INFO 18164 --- [main] o.s.c.s.b.k.p.KafkaTopicProvisioner      : Auto creation of topics is disabled.
2020-06-24 17:27:23.546  INFO 18164 --- [kafka-admin-client-thread | adminclient-1] o.a.k.c.a.i.AdminMetadataManager         : [AdminClient clientId=adminclient-1] Metadata update failed

org.apache.kafka.common.errors.TimeoutException: Call(callName=fetchMetadata, deadlineMs=1593008873529) timed out at 9223372036854775807 after 1 attempt(s)
Caused by: org.apache.kafka.common.errors.TimeoutException: Timed out waiting to send the call.

2020-06-24 17:27:23.557  INFO 18164 --- [main] o.a.k.clients.producer.ProducerConfig    : ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:19093, localhost:19094]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class io.confluent.kafka.serializers.KafkaAvroSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class io.confluent.kafka.serializers.KafkaAvroSerializer

2020-06-24 17:27:23.559  INFO 18164 --- [main] i.c.k.s.KafkaAvroSerializerConfig        : KafkaAvroSerializerConfig values: 
	bearer.auth.token = [hidden]
	proxy.port = -1
	schema.reflection = false
	auto.register.schemas = true
	max.schemas.per.subject = 1000
	basic.auth.credentials.source = URL
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	schema.registry.url = [http://localhost:8081]
	basic.auth.user.info = [hidden]
	proxy.host = 
	schema.registry.basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2020-06-24 17:27:23.559  INFO 18164 --- [main] i.c.k.s.KafkaAvroSerializerConfig        : KafkaAvroSerializerConfig values: 
	bearer.auth.token = [hidden]
	proxy.port = -1
	schema.reflection = false
	auto.register.schemas = true
	max.schemas.per.subject = 1000
	basic.auth.credentials.source = URL
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	schema.registry.url = [http://localhost:8081]
	basic.auth.user.info = [hidden]
	proxy.host = 
	schema.registry.basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2020-06-24 17:27:23.583  INFO 18164 --- [main] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 2.5.0
2020-06-24 17:27:23.584  INFO 18164 --- [main] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: 66563e712b0b9f84
2020-06-24 17:27:23.584  INFO 18164 --- [main] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1593008843583
2020-06-24 17:27:23.828  INFO 18164 --- [kafka-producer-network-thread | producer-1] org.apache.kafka.clients.Metadata        : [Producer clientId=producer-1] Cluster ID: mxWWohzPSwWJyuH0PA9Ofg
2020-06-24 17:27:23.829  INFO 18164 --- [main] o.a.k.clients.producer.KafkaProducer     : [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
2020-06-24 17:27:23.838  INFO 18164 --- [main] o.s.c.s.m.DirectWithAttributesChannel    : Channel 'application.output' has 1 subscriber(s).
2020-06-24 17:27:23.839  INFO 18164 --- [main] o.s.c.s.binder.DefaultBinderFactory      : Retrieving cached binder: kstream
2020-06-24 17:27:23.845  INFO 18164 --- [main] o.s.c.s.b.k.p.KafkaTopicProvisioner      : Using kafka topic for outbound: inventory
2020-06-24 17:27:23.845  INFO 18164 --- [main] o.a.k.clients.admin.AdminClientConfig    : AdminClientConfig values: 
	bootstrap.servers = [localhost:19093, localhost:19094]
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2020-06-24 17:27:23.848  INFO 18164 --- [main] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 2.5.0
2020-06-24 17:27:23.848  INFO 18164 --- [main] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: 66563e712b0b9f84
2020-06-24 17:27:23.849  INFO 18164 --- [main] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1593008843848
2020-06-24 17:27:23.849  INFO 18164 --- [main] o.s.c.s.b.k.p.KafkaTopicProvisioner      : Auto creation of topics is disabled.
2020-06-24 17:27:23.851  INFO 18164 --- [main] i.c.k.s.KafkaAvroSerializerConfig        : KafkaAvroSerializerConfig values: 
	bearer.auth.token = [hidden]
	proxy.port = -1
	schema.reflection = false
	auto.register.schemas = true
	max.schemas.per.subject = 1000
	basic.auth.credentials.source = URL
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	schema.registry.url = [http://localhost:8081]
	basic.auth.user.info = [hidden]
	proxy.host = 
	schema.registry.basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2020-06-24 17:27:23.851  INFO 18164 --- [main] i.c.k.s.KafkaAvroDeserializerConfig      : KafkaAvroDeserializerConfig values: 
	bearer.auth.token = [hidden]
	proxy.port = -1
	schema.reflection = false
	auto.register.schemas = true
	max.schemas.per.subject = 1000
	basic.auth.credentials.source = URL
	specific.avro.reader = false
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	schema.registry.url = [http://localhost:8081]
	basic.auth.user.info = [hidden]
	proxy.host = 
	schema.registry.basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2020-06-24 17:27:23.851  INFO 18164 --- [main] o.s.c.s.b.kafka.streams.KStreamBinder    : Key Serde used for (outbound) inventory: io.confluent.kafka.streams.serdes.avro.PrimitiveAvroSerde
2020-06-24 17:27:23.851  INFO 18164 --- [main] i.c.k.s.KafkaAvroSerializerConfig        : KafkaAvroSerializerConfig values: 
	bearer.auth.token = [hidden]
	proxy.port = -1
	schema.reflection = false
	auto.register.schemas = true
	max.schemas.per.subject = 1000
	basic.auth.credentials.source = URL
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	schema.registry.url = [http://localhost:8081]
	basic.auth.user.info = [hidden]
	proxy.host = 
	schema.registry.basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2020-06-24 17:27:23.851  INFO 18164 --- [main] i.c.k.s.KafkaAvroDeserializerConfig      : KafkaAvroDeserializerConfig values: 
	bearer.auth.token = [hidden]
	proxy.port = -1
	schema.reflection = false
	auto.register.schemas = true
	max.schemas.per.subject = 1000
	basic.auth.credentials.source = URL
	specific.avro.reader = true
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	schema.registry.url = [http://localhost:8081]
	basic.auth.user.info = [hidden]
	proxy.host = 
	schema.registry.basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2020-06-24 17:27:23.852  INFO 18164 --- [main] o.s.c.s.b.kafka.streams.KStreamBinder    : Value Serde used for (outbound) inventory: io.confluent.kafka.streams.serdes.avro.SpecificAvroSerde
2020-06-24 17:27:23.852  INFO 18164 --- [main] o.s.c.s.b.kafka.streams.KStreamBinder    : Native encoding is enabled for inventory. Outbound serialization done at the broker.
2020-06-24 17:27:23.853  INFO 18164 --- [main] o.s.c.s.binder.DefaultBinderFactory      : Retrieving cached binder: kstream
2020-06-24 17:27:23.868  INFO 18164 --- [main] o.a.k.clients.admin.AdminClientConfig    : AdminClientConfig values: 
	bootstrap.servers = [localhost:19093, localhost:19094]
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2020-06-24 17:27:23.870  INFO 18164 --- [main] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 2.5.0
2020-06-24 17:27:23.870  INFO 18164 --- [main] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: 66563e712b0b9f84
2020-06-24 17:27:23.871  INFO 18164 --- [main] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1593008843870
2020-06-24 17:27:23.871  INFO 18164 --- [main] o.s.c.s.b.k.p.KafkaTopicProvisioner      : Auto creation of topics is disabled.
2020-06-24 17:27:23.872  INFO 18164 --- [main] o.s.c.s.binder.DefaultBinderFactory      : Retrieving cached binder: kstream
2020-06-24 17:27:23.873  INFO 18164 --- [main] o.a.k.clients.admin.AdminClientConfig    : AdminClientConfig values: 
	bootstrap.servers = [localhost:19093, localhost:19094]
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2020-06-24 17:27:23.875  INFO 18164 --- [main] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 2.5.0
2020-06-24 17:27:23.876  INFO 18164 --- [main] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: 66563e712b0b9f84
2020-06-24 17:27:23.876  INFO 18164 --- [main] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1593008843875
2020-06-24 17:27:23.876  INFO 18164 --- [main] o.s.c.s.b.k.p.KafkaTopicProvisioner      : Auto creation of topics is disabled.
2020-06-24 17:27:23.877  INFO 18164 --- [main] o.s.c.s.binder.DefaultBinderFactory      : Retrieving cached binder: kstream
2020-06-24 17:27:23.878  INFO 18164 --- [main] o.a.k.clients.admin.AdminClientConfig    : AdminClientConfig values: 
	bootstrap.servers = [localhost:19093, localhost:19094]
	client.dns.lookup = default
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2020-06-24 17:27:23.881  INFO 18164 --- [main] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 2.5.0
2020-06-24 17:27:23.881  INFO 18164 --- [main] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: 66563e712b0b9f84
2020-06-24 17:27:23.881  INFO 18164 --- [main] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1593008843881
2020-06-24 17:27:23.881  INFO 18164 --- [main] o.s.c.s.b.k.p.KafkaTopicProvisioner      : Auto creation of topics is disabled.
2020-06-24 17:27:23.888  INFO 18164 --- [main] org.apache.kafka.streams.StreamsConfig   : StreamsConfig values: 
	application.id = processItem-applicationId
	application.server = 
	bootstrap.servers = [localhost:19093, localhost:19094]
	buffered.records.per.partition = 1000
	built.in.metrics.version = latest
	cache.max.bytes.buffering = 10485760
	client.id = 
	commit.interval.ms = 30000
	connections.max.idle.ms = 540000
	default.deserialization.exception.handler = class org.apache.kafka.streams.errors.LogAndFailExceptionHandler
	default.key.serde = class io.confluent.kafka.streams.serdes.avro.PrimitiveAvroSerde
	default.production.exception.handler = class org.apache.kafka.streams.errors.DefaultProductionExceptionHandler
	default.timestamp.extractor = class org.apache.kafka.streams.processor.FailOnInvalidTimestamp
	default.value.serde = class io.confluent.kafka.streams.serdes.avro.SpecificAvroSerde
	max.task.idle.ms = 0
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	processing.guarantee = at_least_once
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	replication.factor = 1
	request.timeout.ms = 40000
	retries = 0
	retry.backoff.ms = 100
	rocksdb.config.setter = null
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	state.cleanup.delay.ms = 600000
	state.dir = /tmp/kafka-streams
	topology.optimization = none
	upgrade.from = null
	windowstore.changelog.additional.retention.ms = 86400000

2020-06-24 17:27:23.894  INFO 18164 --- [main] org.apache.kafka.streams.KafkaStreams    : stream-client [processItem-applicationId-6ed3bdd5-4741-487b-b6da-d1698a76b4a3] Kafka Streams version: 2.5.0
2020-06-24 17:27:23.894  INFO 18164 --- [main] org.apache.kafka.streams.KafkaStreams    : stream-client [processItem-applicationId-6ed3bdd5-4741-487b-b6da-d1698a76b4a3] Kafka Streams commit ID: 66563e712b0b9f84
2020-06-24 17:27:23.900  INFO 18164 --- [main] o.a.k.clients.admin.AdminClientConfig    : AdminClientConfig values: 
	bootstrap.servers = [localhost:19093, localhost:19094]
	client.dns.lookup = default
	client.id = processItem-applicationId-6ed3bdd5-4741-487b-b6da-d1698a76b4a3-admin
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2020-06-24 17:27:23.902  WARN 18164 --- [main] o.a.k.clients.admin.AdminClientConfig    : The configuration 'schema.registry.url' was supplied but isn't a known config.
2020-06-24 17:27:23.902  INFO 18164 --- [main] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 2.5.0
2020-06-24 17:27:23.902  INFO 18164 --- [main] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: 66563e712b0b9f84
2020-06-24 17:27:23.902  INFO 18164 --- [main] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1593008843902
2020-06-24 17:27:23.903  INFO 18164 --- [main] o.a.k.s.p.internals.StreamThread         : stream-thread [processItem-applicationId-6ed3bdd5-4741-487b-b6da-d1698a76b4a3-StreamThread-1] Creating restore consumer client
2020-06-24 17:27:23.905  INFO 18164 --- [main] o.a.k.clients.consumer.ConsumerConfig    : ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [localhost:19093, localhost:19094]
	check.crcs = true
	client.dns.lookup = default
	client.id = processItem-applicationId-6ed3bdd5-4741-487b-b6da-d1698a76b4a3-StreamThread-1-restore-consumer
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-06-24 17:27:23.923  WARN 18164 --- [main] o.a.k.clients.consumer.ConsumerConfig    : The configuration 'schema.registry.url' was supplied but isn't a known config.
2020-06-24 17:27:23.924  INFO 18164 --- [main] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 2.5.0
2020-06-24 17:27:23.924  INFO 18164 --- [main] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: 66563e712b0b9f84
2020-06-24 17:27:23.924  INFO 18164 --- [main] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1593008843923
2020-06-24 17:27:23.925  INFO 18164 --- [main] o.a.k.s.p.internals.StreamThread         : stream-thread [processItem-applicationId-6ed3bdd5-4741-487b-b6da-d1698a76b4a3-StreamThread-1] Creating shared producer client
2020-06-24 17:27:23.925  INFO 18164 --- [main] o.a.k.clients.producer.ProducerConfig    : ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:19093, localhost:19094]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = processItem-applicationId-6ed3bdd5-4741-487b-b6da-d1698a76b4a3-StreamThread-1-producer
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 100
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2020-06-24 17:27:23.928  WARN 18164 --- [main] o.a.k.clients.producer.ProducerConfig    : The configuration 'schema.registry.url' was supplied but isn't a known config.
2020-06-24 17:27:23.929  INFO 18164 --- [main] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 2.5.0
2020-06-24 17:27:23.929  INFO 18164 --- [main] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: 66563e712b0b9f84
2020-06-24 17:27:23.929  INFO 18164 --- [main] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1593008843929
2020-06-24 17:27:23.936  INFO 18164 --- [main] o.a.k.s.p.internals.StreamThread         : stream-thread [processItem-applicationId-6ed3bdd5-4741-487b-b6da-d1698a76b4a3-StreamThread-1] Creating consumer client
2020-06-24 17:27:23.937  INFO 18164 --- [kafka-producer-network-thread | processItem-applicationId-6ed3bdd5-4741-487b-b6da-d1698a76b4a3-StreamThread-1-producer] org.apache.kafka.clients.Metadata        : [Producer clientId=processItem-applicationId-6ed3bdd5-4741-487b-b6da-d1698a76b4a3-StreamThread-1-producer] Cluster ID: mxWWohzPSwWJyuH0PA9Ofg
2020-06-24 17:27:23.938  INFO 18164 --- [main] o.a.k.clients.consumer.ConsumerConfig    : ConsumerConfig values: 
	allow.auto.create.topics = false
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:19093, localhost:19094]
	check.crcs = true
	client.dns.lookup = default
	client.id = processItem-applicationId-6ed3bdd5-4741-487b-b6da-d1698a76b4a3-StreamThread-1-consumer
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = processItem-applicationId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-06-24 17:27:23.945  INFO 18164 --- [main] o.a.k.s.p.i.a.AssignorConfiguration      : stream-thread [processItem-applicationId-6ed3bdd5-4741-487b-b6da-d1698a76b4a3-StreamThread-1-consumer] Cooperative rebalancing enabled now
2020-06-24 17:27:23.954  WARN 18164 --- [main] o.a.k.clients.consumer.ConsumerConfig    : The configuration 'schema.registry.url' was supplied but isn't a known config.
2020-06-24 17:27:23.954  WARN 18164 --- [main] o.a.k.clients.consumer.ConsumerConfig    : The configuration 'admin.retries' was supplied but isn't a known config.
2020-06-24 17:27:23.954  WARN 18164 --- [main] o.a.k.clients.consumer.ConsumerConfig    : The configuration 'admin.retry.backoff.ms' was supplied but isn't a known config.
2020-06-24 17:27:23.954  INFO 18164 --- [main] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 2.5.0
2020-06-24 17:27:23.954  INFO 18164 --- [main] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: 66563e712b0b9f84
2020-06-24 17:27:23.954  INFO 18164 --- [main] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1593008843954
2020-06-24 17:27:23.958  INFO 18164 --- [main] org.apache.kafka.streams.KafkaStreams    : stream-client [processItem-applicationId-6ed3bdd5-4741-487b-b6da-d1698a76b4a3] State transition from CREATED to REBALANCING
2020-06-24 17:27:23.959  INFO 18164 --- [processItem-applicationId-6ed3bdd5-4741-487b-b6da-d1698a76b4a3-StreamThread-1] o.a.k.s.p.internals.StreamThread         : stream-thread [processItem-applicationId-6ed3bdd5-4741-487b-b6da-d1698a76b4a3-StreamThread-1] Starting
2020-06-24 17:27:23.959  INFO 18164 --- [processItem-applicationId-6ed3bdd5-4741-487b-b6da-d1698a76b4a3-StreamThread-1] o.a.k.s.p.internals.StreamThread         : stream-thread [processItem-applicationId-6ed3bdd5-4741-487b-b6da-d1698a76b4a3-StreamThread-1] State transition from CREATED to STARTING
2020-06-24 17:27:23.960  INFO 18164 --- [processItem-applicationId-6ed3bdd5-4741-487b-b6da-d1698a76b4a3-StreamThread-1] o.a.k.clients.consumer.KafkaConsumer     : [Consumer clientId=processItem-applicationId-6ed3bdd5-4741-487b-b6da-d1698a76b4a3-StreamThread-1-consumer, groupId=processItem-applicationId] Subscribed to topic(s): items
2020-06-24 17:27:23.963  INFO 18164 --- [main] org.apache.kafka.streams.StreamsConfig   : StreamsConfig values: 
	application.id = sinkInventory-applicationId
	application.server = 
	bootstrap.servers = [localhost:19093, localhost:19094]
	buffered.records.per.partition = 1000
	built.in.metrics.version = latest
	cache.max.bytes.buffering = 10485760
	client.id = 
	commit.interval.ms = 30000
	connections.max.idle.ms = 540000
	default.deserialization.exception.handler = class org.apache.kafka.streams.errors.LogAndFailExceptionHandler
	default.key.serde = class io.confluent.kafka.streams.serdes.avro.PrimitiveAvroSerde
	default.production.exception.handler = class org.apache.kafka.streams.errors.DefaultProductionExceptionHandler
	default.timestamp.extractor = class org.apache.kafka.streams.processor.FailOnInvalidTimestamp
	default.value.serde = class io.confluent.kafka.streams.serdes.avro.SpecificAvroSerde
	max.task.idle.ms = 0
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	processing.guarantee = at_least_once
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	replication.factor = 1
	request.timeout.ms = 40000
	retries = 0
	retry.backoff.ms = 100
	rocksdb.config.setter = null
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	state.cleanup.delay.ms = 600000
	state.dir = /tmp/kafka-streams
	topology.optimization = none
	upgrade.from = null
	windowstore.changelog.additional.retention.ms = 86400000

2020-06-24 17:27:23.963  INFO 18164 --- [main] org.apache.kafka.streams.KafkaStreams    : stream-client [sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c] Kafka Streams version: 2.5.0
2020-06-24 17:27:23.963  INFO 18164 --- [main] org.apache.kafka.streams.KafkaStreams    : stream-client [sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c] Kafka Streams commit ID: 66563e712b0b9f84
2020-06-24 17:27:23.971  INFO 18164 --- [main] o.a.k.clients.admin.AdminClientConfig    : AdminClientConfig values: 
	bootstrap.servers = [localhost:19093, localhost:19094]
	client.dns.lookup = default
	client.id = sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-admin
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2020-06-24 17:27:23.974  WARN 18164 --- [main] o.a.k.clients.admin.AdminClientConfig    : The configuration 'schema.registry.url' was supplied but isn't a known config.
2020-06-24 17:27:23.974  INFO 18164 --- [main] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 2.5.0
2020-06-24 17:27:23.974  INFO 18164 --- [main] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: 66563e712b0b9f84
2020-06-24 17:27:23.974  INFO 18164 --- [main] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1593008843974
2020-06-24 17:27:23.975  INFO 18164 --- [main] o.a.k.s.p.internals.StreamThread         : stream-thread [sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1] Creating restore consumer client
2020-06-24 17:27:23.975  INFO 18164 --- [main] o.a.k.clients.consumer.ConsumerConfig    : ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [localhost:19093, localhost:19094]
	check.crcs = true
	client.dns.lookup = default
	client.id = sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1-restore-consumer
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-06-24 17:27:23.980  INFO 18164 --- [processItem-applicationId-6ed3bdd5-4741-487b-b6da-d1698a76b4a3-StreamThread-1] org.apache.kafka.clients.Metadata        : [Consumer clientId=processItem-applicationId-6ed3bdd5-4741-487b-b6da-d1698a76b4a3-StreamThread-1-consumer, groupId=processItem-applicationId] Cluster ID: mxWWohzPSwWJyuH0PA9Ofg
2020-06-24 17:27:23.980  WARN 18164 --- [main] o.a.k.clients.consumer.ConsumerConfig    : The configuration 'schema.registry.url' was supplied but isn't a known config.
2020-06-24 17:27:23.981  INFO 18164 --- [main] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 2.5.0
2020-06-24 17:27:23.981  INFO 18164 --- [main] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: 66563e712b0b9f84
2020-06-24 17:27:23.981  INFO 18164 --- [main] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1593008843981
2020-06-24 17:27:23.981  INFO 18164 --- [main] o.a.k.s.p.internals.StreamThread         : stream-thread [sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1] Creating shared producer client
2020-06-24 17:27:23.981  INFO 18164 --- [main] o.a.k.clients.producer.ProducerConfig    : ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:19093, localhost:19094]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1-producer
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 100
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2020-06-24 17:27:23.981  INFO 18164 --- [processItem-applicationId-6ed3bdd5-4741-487b-b6da-d1698a76b4a3-StreamThread-1] o.a.k.c.c.internals.AbstractCoordinator  : [Consumer clientId=processItem-applicationId-6ed3bdd5-4741-487b-b6da-d1698a76b4a3-StreamThread-1-consumer, groupId=processItem-applicationId] Discovered group coordinator localhost:19093 (id: 2147483646 rack: null)
2020-06-24 17:27:23.987  INFO 18164 --- [processItem-applicationId-6ed3bdd5-4741-487b-b6da-d1698a76b4a3-StreamThread-1] o.a.k.c.c.internals.AbstractCoordinator  : [Consumer clientId=processItem-applicationId-6ed3bdd5-4741-487b-b6da-d1698a76b4a3-StreamThread-1-consumer, groupId=processItem-applicationId] (Re-)joining group
2020-06-24 17:27:23.988  WARN 18164 --- [main] o.a.k.clients.producer.ProducerConfig    : The configuration 'schema.registry.url' was supplied but isn't a known config.
2020-06-24 17:27:23.988  INFO 18164 --- [main] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 2.5.0
2020-06-24 17:27:23.988  INFO 18164 --- [main] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: 66563e712b0b9f84
2020-06-24 17:27:23.988  INFO 18164 --- [main] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1593008843988
2020-06-24 17:27:23.989  INFO 18164 --- [main] o.a.k.s.p.internals.StreamThread         : stream-thread [sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1] Creating consumer client
2020-06-24 17:27:23.989  INFO 18164 --- [main] o.a.k.clients.consumer.ConsumerConfig    : ConsumerConfig values: 
	allow.auto.create.topics = false
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:19093, localhost:19094]
	check.crcs = true
	client.dns.lookup = default
	client.id = sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1-consumer
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sinkInventory-applicationId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-06-24 17:27:23.994  INFO 18164 --- [main] o.a.k.s.p.i.a.AssignorConfiguration      : stream-thread [sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1-consumer] Cooperative rebalancing enabled now
2020-06-24 17:27:23.995  WARN 18164 --- [main] o.a.k.clients.consumer.ConsumerConfig    : The configuration 'schema.registry.url' was supplied but isn't a known config.
2020-06-24 17:27:23.995  WARN 18164 --- [main] o.a.k.clients.consumer.ConsumerConfig    : The configuration 'admin.retries' was supplied but isn't a known config.
2020-06-24 17:27:23.995  WARN 18164 --- [main] o.a.k.clients.consumer.ConsumerConfig    : The configuration 'admin.retry.backoff.ms' was supplied but isn't a known config.
2020-06-24 17:27:23.995  INFO 18164 --- [main] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 2.5.0
2020-06-24 17:27:23.995  INFO 18164 --- [main] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: 66563e712b0b9f84
2020-06-24 17:27:23.995  INFO 18164 --- [main] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1593008843995
2020-06-24 17:27:23.996  INFO 18164 --- [main] org.apache.kafka.streams.KafkaStreams    : stream-client [sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c] State transition from CREATED to REBALANCING
2020-06-24 17:27:23.997  INFO 18164 --- [sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1] o.a.k.s.p.internals.StreamThread         : stream-thread [sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1] Starting
2020-06-24 17:27:23.997  INFO 18164 --- [sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1] o.a.k.s.p.internals.StreamThread         : stream-thread [sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1] State transition from CREATED to STARTING
2020-06-24 17:27:23.997  INFO 18164 --- [kafka-producer-network-thread | sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1-producer] org.apache.kafka.clients.Metadata        : [Producer clientId=sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1-producer] Cluster ID: mxWWohzPSwWJyuH0PA9Ofg
2020-06-24 17:27:23.998  INFO 18164 --- [main] org.apache.kafka.streams.StreamsConfig   : StreamsConfig values: 
	application.id = sinkItem-applicationId
	application.server = 
	bootstrap.servers = [localhost:19093, localhost:19094]
	buffered.records.per.partition = 1000
	built.in.metrics.version = latest
	cache.max.bytes.buffering = 10485760
	client.id = 
	commit.interval.ms = 30000
	connections.max.idle.ms = 540000
	default.deserialization.exception.handler = class org.apache.kafka.streams.errors.LogAndFailExceptionHandler
	default.key.serde = class io.confluent.kafka.streams.serdes.avro.PrimitiveAvroSerde
	default.production.exception.handler = class org.apache.kafka.streams.errors.DefaultProductionExceptionHandler
	default.timestamp.extractor = class org.apache.kafka.streams.processor.FailOnInvalidTimestamp
	default.value.serde = class io.confluent.kafka.streams.serdes.avro.SpecificAvroSerde
	max.task.idle.ms = 0
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	processing.guarantee = at_least_once
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	replication.factor = 1
	request.timeout.ms = 40000
	retries = 0
	retry.backoff.ms = 100
	rocksdb.config.setter = null
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	state.cleanup.delay.ms = 600000
	state.dir = /tmp/kafka-streams
	topology.optimization = none
	upgrade.from = null
	windowstore.changelog.additional.retention.ms = 86400000

2020-06-24 17:27:23.998  INFO 18164 --- [sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1] o.a.k.clients.consumer.KafkaConsumer     : [Consumer clientId=sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1-consumer, groupId=sinkInventory-applicationId] Subscribed to topic(s): inventory
2020-06-24 17:27:23.998  INFO 18164 --- [main] org.apache.kafka.streams.KafkaStreams    : stream-client [sinkItem-applicationId-244f698d-c580-4d52-90fa-1cf875a0c1cb] Kafka Streams version: 2.5.0
2020-06-24 17:27:23.998  INFO 18164 --- [main] org.apache.kafka.streams.KafkaStreams    : stream-client [sinkItem-applicationId-244f698d-c580-4d52-90fa-1cf875a0c1cb] Kafka Streams commit ID: 66563e712b0b9f84
2020-06-24 17:27:23.998  INFO 18164 --- [main] o.a.k.clients.admin.AdminClientConfig    : AdminClientConfig values: 
	bootstrap.servers = [localhost:19093, localhost:19094]
	client.dns.lookup = default
	client.id = sinkItem-applicationId-244f698d-c580-4d52-90fa-1cf875a0c1cb-admin
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2020-06-24 17:27:24.003  WARN 18164 --- [main] o.a.k.clients.admin.AdminClientConfig    : The configuration 'schema.registry.url' was supplied but isn't a known config.
2020-06-24 17:27:24.003  INFO 18164 --- [main] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 2.5.0
2020-06-24 17:27:24.003  INFO 18164 --- [main] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: 66563e712b0b9f84
2020-06-24 17:27:24.003  INFO 18164 --- [main] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1593008844003
2020-06-24 17:27:24.004  INFO 18164 --- [main] o.a.k.s.p.internals.StreamThread         : stream-thread [sinkItem-applicationId-244f698d-c580-4d52-90fa-1cf875a0c1cb-StreamThread-1] Creating restore consumer client
2020-06-24 17:27:24.004  INFO 18164 --- [main] o.a.k.clients.consumer.ConsumerConfig    : ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [localhost:19093, localhost:19094]
	check.crcs = true
	client.dns.lookup = default
	client.id = sinkItem-applicationId-244f698d-c580-4d52-90fa-1cf875a0c1cb-StreamThread-1-restore-consumer
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-06-24 17:27:24.008  WARN 18164 --- [main] o.a.k.clients.consumer.ConsumerConfig    : The configuration 'schema.registry.url' was supplied but isn't a known config.
2020-06-24 17:27:24.009  INFO 18164 --- [main] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 2.5.0
2020-06-24 17:27:24.009  INFO 18164 --- [main] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: 66563e712b0b9f84
2020-06-24 17:27:24.009  INFO 18164 --- [main] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1593008844009
2020-06-24 17:27:24.009  INFO 18164 --- [main] o.a.k.s.p.internals.StreamThread         : stream-thread [sinkItem-applicationId-244f698d-c580-4d52-90fa-1cf875a0c1cb-StreamThread-1] Creating shared producer client
2020-06-24 17:27:24.009  INFO 18164 --- [main] o.a.k.clients.producer.ProducerConfig    : ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:19093, localhost:19094]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = sinkItem-applicationId-244f698d-c580-4d52-90fa-1cf875a0c1cb-StreamThread-1-producer
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 100
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2020-06-24 17:27:24.011  INFO 18164 --- [sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1] org.apache.kafka.clients.Metadata        : [Consumer clientId=sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1-consumer, groupId=sinkInventory-applicationId] Cluster ID: mxWWohzPSwWJyuH0PA9Ofg
2020-06-24 17:27:24.012  INFO 18164 --- [sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1] o.a.k.c.c.internals.AbstractCoordinator  : [Consumer clientId=sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1-consumer, groupId=sinkInventory-applicationId] Discovered group coordinator localhost:19094 (id: 2147483645 rack: null)
2020-06-24 17:27:24.013  INFO 18164 --- [sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1] o.a.k.c.c.internals.AbstractCoordinator  : [Consumer clientId=sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1-consumer, groupId=sinkInventory-applicationId] (Re-)joining group
2020-06-24 17:27:24.017  WARN 18164 --- [main] o.a.k.clients.producer.ProducerConfig    : The configuration 'schema.registry.url' was supplied but isn't a known config.
2020-06-24 17:27:24.017  INFO 18164 --- [main] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 2.5.0
2020-06-24 17:27:24.017  INFO 18164 --- [main] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: 66563e712b0b9f84
2020-06-24 17:27:24.017  INFO 18164 --- [main] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1593008844017
2020-06-24 17:27:24.017  INFO 18164 --- [main] o.a.k.s.p.internals.StreamThread         : stream-thread [sinkItem-applicationId-244f698d-c580-4d52-90fa-1cf875a0c1cb-StreamThread-1] Creating consumer client
2020-06-24 17:27:24.018  INFO 18164 --- [main] o.a.k.clients.consumer.ConsumerConfig    : ConsumerConfig values: 
	allow.auto.create.topics = false
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:19093, localhost:19094]
	check.crcs = true
	client.dns.lookup = default
	client.id = sinkItem-applicationId-244f698d-c580-4d52-90fa-1cf875a0c1cb-StreamThread-1-consumer
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sinkItem-applicationId
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-06-24 17:27:24.022  INFO 18164 --- [processItem-applicationId-6ed3bdd5-4741-487b-b6da-d1698a76b4a3-StreamThread-1] o.a.k.c.c.internals.AbstractCoordinator  : [Consumer clientId=processItem-applicationId-6ed3bdd5-4741-487b-b6da-d1698a76b4a3-StreamThread-1-consumer, groupId=processItem-applicationId] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-06-24 17:27:24.022  INFO 18164 --- [processItem-applicationId-6ed3bdd5-4741-487b-b6da-d1698a76b4a3-StreamThread-1] o.a.k.c.c.internals.AbstractCoordinator  : [Consumer clientId=processItem-applicationId-6ed3bdd5-4741-487b-b6da-d1698a76b4a3-StreamThread-1-consumer, groupId=processItem-applicationId] (Re-)joining group
2020-06-24 17:27:24.024  INFO 18164 --- [main] o.a.k.s.p.i.a.AssignorConfiguration      : stream-thread [sinkItem-applicationId-244f698d-c580-4d52-90fa-1cf875a0c1cb-StreamThread-1-consumer] Cooperative rebalancing enabled now
2020-06-24 17:27:24.026  WARN 18164 --- [main] o.a.k.clients.consumer.ConsumerConfig    : The configuration 'schema.registry.url' was supplied but isn't a known config.
2020-06-24 17:27:24.026  WARN 18164 --- [main] o.a.k.clients.consumer.ConsumerConfig    : The configuration 'admin.retries' was supplied but isn't a known config.
2020-06-24 17:27:24.026  WARN 18164 --- [main] o.a.k.clients.consumer.ConsumerConfig    : The configuration 'admin.retry.backoff.ms' was supplied but isn't a known config.
2020-06-24 17:27:24.026  INFO 18164 --- [main] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 2.5.0
2020-06-24 17:27:24.026  INFO 18164 --- [main] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: 66563e712b0b9f84
2020-06-24 17:27:24.026  INFO 18164 --- [main] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1593008844026
2020-06-24 17:27:24.027  INFO 18164 --- [kafka-producer-network-thread | sinkItem-applicationId-244f698d-c580-4d52-90fa-1cf875a0c1cb-StreamThread-1-producer] org.apache.kafka.clients.Metadata        : [Producer clientId=sinkItem-applicationId-244f698d-c580-4d52-90fa-1cf875a0c1cb-StreamThread-1-producer] Cluster ID: mxWWohzPSwWJyuH0PA9Ofg
2020-06-24 17:27:24.027  INFO 18164 --- [sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1] o.a.k.c.c.internals.AbstractCoordinator  : [Consumer clientId=sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1-consumer, groupId=sinkInventory-applicationId] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-06-24 17:27:24.027  INFO 18164 --- [main] org.apache.kafka.streams.KafkaStreams    : stream-client [sinkItem-applicationId-244f698d-c580-4d52-90fa-1cf875a0c1cb] State transition from CREATED to REBALANCING
2020-06-24 17:27:24.028  INFO 18164 --- [sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1] o.a.k.c.c.internals.AbstractCoordinator  : [Consumer clientId=sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1-consumer, groupId=sinkInventory-applicationId] (Re-)joining group
2020-06-24 17:27:24.028  INFO 18164 --- [sinkItem-applicationId-244f698d-c580-4d52-90fa-1cf875a0c1cb-StreamThread-1] o.a.k.s.p.internals.StreamThread         : stream-thread [sinkItem-applicationId-244f698d-c580-4d52-90fa-1cf875a0c1cb-StreamThread-1] Starting
2020-06-24 17:27:24.028  INFO 18164 --- [sinkItem-applicationId-244f698d-c580-4d52-90fa-1cf875a0c1cb-StreamThread-1] o.a.k.s.p.internals.StreamThread         : stream-thread [sinkItem-applicationId-244f698d-c580-4d52-90fa-1cf875a0c1cb-StreamThread-1] State transition from CREATED to STARTING
2020-06-24 17:27:24.029  INFO 18164 --- [sinkItem-applicationId-244f698d-c580-4d52-90fa-1cf875a0c1cb-StreamThread-1] o.a.k.clients.consumer.KafkaConsumer     : [Consumer clientId=sinkItem-applicationId-244f698d-c580-4d52-90fa-1cf875a0c1cb-StreamThread-1-consumer, groupId=sinkItem-applicationId] Subscribed to topic(s): items
2020-06-24 17:27:24.043  INFO 18164 --- [processItem-applicationId-6ed3bdd5-4741-487b-b6da-d1698a76b4a3-StreamThread-1] o.a.k.s.p.i.StreamsPartitionAssignor     : stream-thread [processItem-applicationId-6ed3bdd5-4741-487b-b6da-d1698a76b4a3-StreamThread-1-consumer] Assigned tasks to clients as 
6ed3bdd5-4741-487b-b6da-d1698a76b4a3=[activeTasks: ([0_0, 0_1]) standbyTasks: ([]) assignedTasks: ([0_0, 0_1]) prevActiveTasks: ([]) prevStandbyTasks: ([]) prevAssignedTasks: ([]) prevOwnedPartitionsByConsumerId: ([]) capacity: 1].
2020-06-24 17:27:24.049  INFO 18164 --- [sinkItem-applicationId-244f698d-c580-4d52-90fa-1cf875a0c1cb-StreamThread-1] org.apache.kafka.clients.Metadata        : [Consumer clientId=sinkItem-applicationId-244f698d-c580-4d52-90fa-1cf875a0c1cb-StreamThread-1-consumer, groupId=sinkItem-applicationId] Cluster ID: mxWWohzPSwWJyuH0PA9Ofg
2020-06-24 17:27:24.050  INFO 18164 --- [processItem-applicationId-6ed3bdd5-4741-487b-b6da-d1698a76b4a3-StreamThread-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=processItem-applicationId-6ed3bdd5-4741-487b-b6da-d1698a76b4a3-StreamThread-1-consumer, groupId=processItem-applicationId] Finished assignment for group at generation 25: {processItem-applicationId-6ed3bdd5-4741-487b-b6da-d1698a76b4a3-StreamThread-1-consumer-8e7e7e64-19dc-41aa-ae1e-965364ad4919=Assignment(partitions=[items-0, items-1], userDataSize=48)}
2020-06-24 17:27:24.053  INFO 18164 --- [sinkItem-applicationId-244f698d-c580-4d52-90fa-1cf875a0c1cb-StreamThread-1] o.a.k.c.c.internals.AbstractCoordinator  : [Consumer clientId=sinkItem-applicationId-244f698d-c580-4d52-90fa-1cf875a0c1cb-StreamThread-1-consumer, groupId=sinkItem-applicationId] Discovered group coordinator localhost:19093 (id: 2147483646 rack: null)
2020-06-24 17:27:24.055  INFO 18164 --- [sinkItem-applicationId-244f698d-c580-4d52-90fa-1cf875a0c1cb-StreamThread-1] o.a.k.c.c.internals.AbstractCoordinator  : [Consumer clientId=sinkItem-applicationId-244f698d-c580-4d52-90fa-1cf875a0c1cb-StreamThread-1-consumer, groupId=sinkItem-applicationId] (Re-)joining group
2020-06-24 17:27:24.059  INFO 18164 --- [main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat started on port(s): 9000 (http) with context path ''
2020-06-24 17:27:24.065  INFO 18164 --- [processItem-applicationId-6ed3bdd5-4741-487b-b6da-d1698a76b4a3-StreamThread-1] o.a.k.c.c.internals.AbstractCoordinator  : [Consumer clientId=processItem-applicationId-6ed3bdd5-4741-487b-b6da-d1698a76b4a3-StreamThread-1-consumer, groupId=processItem-applicationId] Successfully joined group with generation 25
2020-06-24 17:27:24.067  INFO 18164 --- [processItem-applicationId-6ed3bdd5-4741-487b-b6da-d1698a76b4a3-StreamThread-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=processItem-applicationId-6ed3bdd5-4741-487b-b6da-d1698a76b4a3-StreamThread-1-consumer, groupId=processItem-applicationId] Updating assignment with
now assigned partitions: items-0, items-1
compare with previously owned partitions: 
newly added partitions: items-0, items-1
revoked partitions: 

2020-06-24 17:27:24.069  INFO 18164 --- [sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1] o.a.k.s.p.i.StreamsPartitionAssignor     : stream-thread [sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1-consumer] Assigned tasks to clients as 
2081c162-1d3f-4879-b815-f6c07d65a20c=[activeTasks: ([0_0, 0_1]) standbyTasks: ([]) assignedTasks: ([0_0, 0_1]) prevActiveTasks: ([]) prevStandbyTasks: ([]) prevAssignedTasks: ([]) prevOwnedPartitionsByConsumerId: ([]) capacity: 1].
2020-06-24 17:27:24.069  INFO 18164 --- [sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1-consumer, groupId=sinkInventory-applicationId] Finished assignment for group at generation 33: {sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1-consumer-5a5e4375-5441-4f71-87aa-9396436298b4=Assignment(partitions=[inventory-0, inventory-1], userDataSize=48)}
2020-06-24 17:27:24.069  INFO 18164 --- [sinkItem-applicationId-244f698d-c580-4d52-90fa-1cf875a0c1cb-StreamThread-1] o.a.k.c.c.internals.AbstractCoordinator  : [Consumer clientId=sinkItem-applicationId-244f698d-c580-4d52-90fa-1cf875a0c1cb-StreamThread-1-consumer, groupId=sinkItem-applicationId] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group
2020-06-24 17:27:24.070  INFO 18164 --- [sinkItem-applicationId-244f698d-c580-4d52-90fa-1cf875a0c1cb-StreamThread-1] o.a.k.c.c.internals.AbstractCoordinator  : [Consumer clientId=sinkItem-applicationId-244f698d-c580-4d52-90fa-1cf875a0c1cb-StreamThread-1-consumer, groupId=sinkItem-applicationId] (Re-)joining group
2020-06-24 17:27:24.074 DEBUG 18164 --- [main] ConditionEvaluationReportLoggingListener : 


============================
CONDITIONS EVALUATION REPORT
============================


Positive matches:
-----------------

   AopAutoConfiguration matched:
      - @ConditionalOnProperty (spring.aop.auto=true) matched (OnPropertyCondition)

   AopAutoConfiguration.ClassProxyingConfiguration matched:
      - @ConditionalOnMissingClass did not find unwanted class 'org.aspectj.weaver.Advice' (OnClassCondition)
      - @ConditionalOnProperty (spring.aop.proxy-target-class=true) matched (OnPropertyCondition)

   AvroMessageConverterAutoConfiguration matched:
      - @ConditionalOnClass found required class 'org.apache.avro.Schema' (OnClassCondition)
      - @ConditionalOnProperty (spring.cloud.schemaRegistryClient.enabled) matched (OnPropertyCondition)
      - @ConditionalOnBean (types: org.springframework.cloud.schema.registry.client.SchemaRegistryClient; SearchStrategy: all) found bean 'schemaRegistryClient' (OnBeanCondition)

   AvroMessageConverterAutoConfiguration#avroSchemaMessageConverter matched:
      - @ConditionalOnMissingBean (types: org.springframework.cloud.schema.registry.avro.AvroSchemaRegistryClientMessageConverter; SearchStrategy: all) did not find any beans (OnBeanCondition)

   AvroMessageConverterAutoConfiguration#cacheManager matched:
      - @ConditionalOnMissingBean (types: org.springframework.cache.CacheManager; SearchStrategy: all) did not find any beans (OnBeanCondition)

   BindingServiceConfiguration matched:
      - @ConditionalOnBean (types: org.springframework.cloud.stream.binder.BinderTypeRegistry; SearchStrategy: current) found bean 'binderTypeRegistry' (OnBeanCondition)

   BindingServiceConfiguration#binderAwareRouterBeanPostProcessor matched:
      - @ConditionalOnMissingBean (types: org.springframework.cloud.stream.binding.BinderAwareRouter; SearchStrategy: all) did not find any beans (OnBeanCondition)

   BindingServiceConfiguration#binderFactory matched:
      - @ConditionalOnMissingBean (types: org.springframework.cloud.stream.binder.BinderFactory; SearchStrategy: all) did not find any beans (OnBeanCondition)

   BindingServiceConfiguration#bindingService matched:
      - @ConditionalOnMissingBean (types: org.springframework.cloud.stream.binding.BindingService; SearchStrategy: current) did not find any beans (OnBeanCondition)

   BindingServiceConfiguration#streamListenerAnnotationBeanPostProcessor matched:
      - @ConditionalOnMissingBean (types: org.springframework.cloud.stream.binding.StreamListenerAnnotationBeanPostProcessor; SearchStrategy: current) did not find any beans (OnBeanCondition)

   ChannelBindingAutoConfiguration matched:
      - @ConditionalOnBean (types: org.springframework.cloud.stream.binding.BindingService; SearchStrategy: all) found bean 'bindingService' (OnBeanCondition)

   ChannelBindingAutoConfiguration#defaultPoller matched:
      - @ConditionalOnMissingBean (types: org.springframework.integration.scheduling.PollerMetadata; SearchStrategy: all) did not find any beans (OnBeanCondition)

   ContextFunctionCatalogAutoConfiguration matched:
      - @ConditionalOnMissingBean (types: org.springframework.cloud.function.context.FunctionCatalog; SearchStrategy: all) did not find any beans (OnBeanCondition)

   ContextFunctionCatalogAutoConfiguration.JacksonConfiguration matched:
      - @ConditionalOnClass found required class 'com.fasterxml.jackson.databind.ObjectMapper' (OnClassCondition)
      - @ConditionalOnProperty (spring.http.converters.preferred-json-mapper=jackson) matched (OnPropertyCondition)

   ContextFunctionCatalogAutoConfiguration.PlainFunctionScanConfiguration matched:
      - @ConditionalOnProperty (spring.cloud.function.scan.enabled=true) matched (OnPropertyCondition)

   DispatcherServletAutoConfiguration matched:
      - @ConditionalOnClass found required class 'org.springframework.web.servlet.DispatcherServlet' (OnClassCondition)
      - found 'session' scope (OnWebApplicationCondition)

   DispatcherServletAutoConfiguration.DispatcherServletConfiguration matched:
      - @ConditionalOnClass found required class 'javax.servlet.ServletRegistration' (OnClassCondition)
      - Default DispatcherServlet did not find dispatcher servlet beans (DispatcherServletAutoConfiguration.DefaultDispatcherServletCondition)

   DispatcherServletAutoConfiguration.DispatcherServletRegistrationConfiguration matched:
      - @ConditionalOnClass found required class 'javax.servlet.ServletRegistration' (OnClassCondition)
      - DispatcherServlet Registration did not find servlet registration bean (DispatcherServletAutoConfiguration.DispatcherServletRegistrationCondition)

   DispatcherServletAutoConfiguration.DispatcherServletRegistrationConfiguration#dispatcherServletRegistration matched:
      - @ConditionalOnBean (names: dispatcherServlet types: org.springframework.web.servlet.DispatcherServlet; SearchStrategy: all) found bean 'dispatcherServlet' (OnBeanCondition)

   EmbeddedWebServerFactoryCustomizerAutoConfiguration matched:
      - @ConditionalOnWebApplication (required) found 'session' scope (OnWebApplicationCondition)

   EmbeddedWebServerFactoryCustomizerAutoConfiguration.TomcatWebServerFactoryCustomizerConfiguration matched:
      - @ConditionalOnClass found required classes 'org.apache.catalina.startup.Tomcat', 'org.apache.coyote.UpgradeProtocol' (OnClassCondition)

   ErrorMvcAutoConfiguration matched:
      - @ConditionalOnClass found required classes 'javax.servlet.Servlet', 'org.springframework.web.servlet.DispatcherServlet' (OnClassCondition)
      - found 'session' scope (OnWebApplicationCondition)

   ErrorMvcAutoConfiguration#basicErrorController matched:
      - @ConditionalOnMissingBean (types: org.springframework.boot.web.servlet.error.ErrorController; SearchStrategy: current) did not find any beans (OnBeanCondition)

   ErrorMvcAutoConfiguration#errorAttributes matched:
      - @ConditionalOnMissingBean (types: org.springframework.boot.web.servlet.error.ErrorAttributes; SearchStrategy: current) did not find any beans (OnBeanCondition)

   ErrorMvcAutoConfiguration.DefaultErrorViewResolverConfiguration#conventionErrorViewResolver matched:
      - @ConditionalOnBean (types: org.springframework.web.servlet.DispatcherServlet; SearchStrategy: all) found bean 'dispatcherServlet'; @ConditionalOnMissingBean (types: org.springframework.boot.autoconfigure.web.servlet.error.ErrorViewResolver; SearchStrategy: all) did not find any beans (OnBeanCondition)

   ErrorMvcAutoConfiguration.WhitelabelErrorViewConfiguration matched:
      - @ConditionalOnProperty (server.error.whitelabel.enabled) matched (OnPropertyCondition)
      - ErrorTemplate Missing did not find error template view (ErrorMvcAutoConfiguration.ErrorTemplateMissingCondition)

   ErrorMvcAutoConfiguration.WhitelabelErrorViewConfiguration#beanNameViewResolver matched:
      - @ConditionalOnMissingBean (types: org.springframework.web.servlet.view.BeanNameViewResolver; SearchStrategy: all) did not find any beans (OnBeanCondition)

   ErrorMvcAutoConfiguration.WhitelabelErrorViewConfiguration#defaultErrorView matched:
      - @ConditionalOnMissingBean (names: error; SearchStrategy: all) did not find any beans (OnBeanCondition)

   FunctionConfiguration matched:
      - @ConditionalOnBean (types: org.springframework.cloud.function.context.FunctionRegistry; SearchStrategy: all) found bean 'functionCatalog' (OnBeanCondition)

   GenericCacheConfiguration matched:
      - Cache org.springframework.boot.autoconfigure.cache.GenericCacheConfiguration automatic cache type (CacheCondition)

   HttpEncodingAutoConfiguration matched:
      - @ConditionalOnClass found required class 'org.springframework.web.filter.CharacterEncodingFilter' (OnClassCondition)
      - found 'session' scope (OnWebApplicationCondition)
      - @ConditionalOnProperty (server.servlet.encoding.enabled) matched (OnPropertyCondition)

   HttpEncodingAutoConfiguration#characterEncodingFilter matched:
      - @ConditionalOnMissingBean (types: org.springframework.web.filter.CharacterEncodingFilter; SearchStrategy: all) did not find any beans (OnBeanCondition)

   HttpMessageConvertersAutoConfiguration matched:
      - @ConditionalOnClass found required class 'org.springframework.http.converter.HttpMessageConverter' (OnClassCondition)
      - NoneNestedConditions 0 matched 1 did not; NestedCondition on HttpMessageConvertersAutoConfiguration.NotReactiveWebApplicationCondition.ReactiveWebApplication did not find reactive web application classes (HttpMessageConvertersAutoConfiguration.NotReactiveWebApplicationCondition)

   HttpMessageConvertersAutoConfiguration#messageConverters matched:
      - @ConditionalOnMissingBean (types: org.springframework.boot.autoconfigure.http.HttpMessageConverters; SearchStrategy: all) did not find any beans (OnBeanCondition)

   HttpMessageConvertersAutoConfiguration.StringHttpMessageConverterConfiguration matched:
      - @ConditionalOnClass found required class 'org.springframework.http.converter.StringHttpMessageConverter' (OnClassCondition)

   HttpMessageConvertersAutoConfiguration.StringHttpMessageConverterConfiguration#stringHttpMessageConverter matched:
      - @ConditionalOnMissingBean (types: org.springframework.http.converter.StringHttpMessageConverter; SearchStrategy: all) did not find any beans (OnBeanCondition)

   IntegrationAutoConfiguration matched:
      - @ConditionalOnClass found required class 'org.springframework.integration.config.EnableIntegration' (OnClassCondition)

   IntegrationAutoConfiguration.IntegrationComponentScanConfiguration matched:
      - @ConditionalOnMissingBean (types: org.springframework.integration.gateway.GatewayProxyFactoryBean; SearchStrategy: all) did not find any beans (OnBeanCondition)

   IntegrationAutoConfiguration.IntegrationJmxConfiguration matched:
      - @ConditionalOnClass found required class 'org.springframework.integration.jmx.config.EnableIntegrationMBeanExport' (OnClassCondition)
      - @ConditionalOnProperty (spring.jmx.enabled=true) matched (OnPropertyCondition)
      - @ConditionalOnBean (types: javax.management.MBeanServer; SearchStrategy: all) found bean 'mbeanServer'; @ConditionalOnMissingBean (types: org.springframework.integration.monitor.IntegrationMBeanExporter; SearchStrategy: current) did not find any beans (OnBeanCondition)

   IntegrationAutoConfiguration.IntegrationManagementConfiguration matched:
      - @ConditionalOnClass found required class 'org.springframework.integration.config.EnableIntegrationManagement' (OnClassCondition)
      - @ConditionalOnMissingBean (names: integrationManagementConfigurer types: org.springframework.integration.config.IntegrationManagementConfigurer; SearchStrategy: current) did not find any beans (OnBeanCondition)

   JacksonAutoConfiguration matched:
      - @ConditionalOnClass found required class 'com.fasterxml.jackson.databind.ObjectMapper' (OnClassCondition)

   JacksonAutoConfiguration.Jackson2ObjectMapperBuilderCustomizerConfiguration matched:
      - @ConditionalOnClass found required class 'org.springframework.http.converter.json.Jackson2ObjectMapperBuilder' (OnClassCondition)

   JacksonAutoConfiguration.JacksonObjectMapperBuilderConfiguration matched:
      - @ConditionalOnClass found required class 'org.springframework.http.converter.json.Jackson2ObjectMapperBuilder' (OnClassCondition)

   JacksonAutoConfiguration.JacksonObjectMapperBuilderConfiguration#jacksonObjectMapperBuilder matched:
      - @ConditionalOnMissingBean (types: org.springframework.http.converter.json.Jackson2ObjectMapperBuilder; SearchStrategy: all) did not find any beans (OnBeanCondition)

   JacksonAutoConfiguration.JacksonObjectMapperConfiguration matched:
      - @ConditionalOnClass found required class 'org.springframework.http.converter.json.Jackson2ObjectMapperBuilder' (OnClassCondition)

   JacksonAutoConfiguration.JacksonObjectMapperConfiguration#jacksonObjectMapper matched:
      - @ConditionalOnMissingBean (types: com.fasterxml.jackson.databind.ObjectMapper; SearchStrategy: all) did not find any beans (OnBeanCondition)

   JacksonAutoConfiguration.ParameterNamesModuleConfiguration matched:
      - @ConditionalOnClass found required class 'com.fasterxml.jackson.module.paramnames.ParameterNamesModule' (OnClassCondition)

   JacksonAutoConfiguration.ParameterNamesModuleConfiguration#parameterNamesModule matched:
      - @ConditionalOnMissingBean (types: com.fasterxml.jackson.module.paramnames.ParameterNamesModule; SearchStrategy: all) did not find any beans (OnBeanCondition)

   JacksonHttpMessageConvertersConfiguration.MappingJackson2HttpMessageConverterConfiguration matched:
      - @ConditionalOnClass found required class 'com.fasterxml.jackson.databind.ObjectMapper' (OnClassCondition)
      - @ConditionalOnProperty (spring.mvc.converters.preferred-json-mapper=jackson) matched (OnPropertyCondition)
      - @ConditionalOnBean (types: com.fasterxml.jackson.databind.ObjectMapper; SearchStrategy: all) found bean 'jacksonObjectMapper' (OnBeanCondition)

   JacksonHttpMessageConvertersConfiguration.MappingJackson2HttpMessageConverterConfiguration#mappingJackson2HttpMessageConverter matched:
      - @ConditionalOnMissingBean (types: org.springframework.http.converter.json.MappingJackson2HttpMessageConverter ignored: org.springframework.hateoas.server.mvc.TypeConstrainedMappingJackson2HttpMessageConverter,org.springframework.data.rest.webmvc.alps.AlpsJsonHttpMessageConverter; SearchStrategy: all) did not find any beans (OnBeanCondition)

   JmxAutoConfiguration matched:
      - @ConditionalOnClass found required class 'org.springframework.jmx.export.MBeanExporter' (OnClassCondition)
      - @ConditionalOnProperty (spring.jmx.enabled=true) matched (OnPropertyCondition)

   JmxAutoConfiguration#mbeanExporter matched:
      - @ConditionalOnMissingBean (types: org.springframework.jmx.export.MBeanExporter; SearchStrategy: current) did not find any beans (OnBeanCondition)

   JmxAutoConfiguration#mbeanServer matched:
      - @ConditionalOnMissingBean (types: javax.management.MBeanServer; SearchStrategy: all) did not find any beans (OnBeanCondition)

   JmxAutoConfiguration#objectNamingStrategy matched:
      - @ConditionalOnMissingBean (types: org.springframework.jmx.export.naming.ObjectNamingStrategy; SearchStrategy: current) did not find any beans (OnBeanCondition)

   KafkaAnnotationDrivenConfiguration matched:
      - @ConditionalOnClass found required class 'org.springframework.kafka.annotation.EnableKafka' (OnClassCondition)

   KafkaAnnotationDrivenConfiguration#kafkaListenerContainerFactory matched:
      - @ConditionalOnMissingBean (names: kafkaListenerContainerFactory; SearchStrategy: all) did not find any beans (OnBeanCondition)

   KafkaAnnotationDrivenConfiguration#kafkaListenerContainerFactoryConfigurer matched:
      - @ConditionalOnMissingBean (types: org.springframework.boot.autoconfigure.kafka.ConcurrentKafkaListenerContainerFactoryConfigurer; SearchStrategy: all) did not find any beans (OnBeanCondition)

   KafkaAnnotationDrivenConfiguration.EnableKafkaConfiguration matched:
      - @ConditionalOnMissingBean (names: org.springframework.kafka.config.internalKafkaListenerAnnotationProcessor; SearchStrategy: all) did not find any beans (OnBeanCondition)

   KafkaAutoConfiguration matched:
      - @ConditionalOnClass found required class 'org.springframework.kafka.core.KafkaTemplate' (OnClassCondition)

   KafkaAutoConfiguration#kafkaAdmin matched:
      - @ConditionalOnMissingBean (types: org.springframework.kafka.core.KafkaAdmin; SearchStrategy: all) did not find any beans (OnBeanCondition)

   KafkaAutoConfiguration#kafkaConsumerFactory matched:
      - @ConditionalOnMissingBean (types: org.springframework.kafka.core.ConsumerFactory; SearchStrategy: all) did not find any beans (OnBeanCondition)

   KafkaAutoConfiguration#kafkaProducerFactory matched:
      - @ConditionalOnMissingBean (types: org.springframework.kafka.core.ProducerFactory; SearchStrategy: all) did not find any beans (OnBeanCondition)

   KafkaAutoConfiguration#kafkaProducerListener matched:
      - @ConditionalOnMissingBean (types: org.springframework.kafka.support.ProducerListener; SearchStrategy: all) did not find any beans (OnBeanCondition)

   KafkaAutoConfiguration#kafkaTemplate matched:
      - @ConditionalOnMissingBean (types: org.springframework.kafka.core.KafkaTemplate; SearchStrategy: all) did not find any beans (OnBeanCondition)

   KafkaStreamsBinderSupportAutoConfiguration matched:
      - @ConditionalOnBean (types: org.springframework.cloud.stream.binding.BindingService; SearchStrategy: all) found bean 'bindingService' (OnBeanCondition)

   KafkaStreamsBinderSupportAutoConfiguration#kafkaStreamsFunctionProcessor matched:
      - Matched. Function/BiFunction/Consumer beans found (FunctionDetectorCondition)

   KafkaStreamsBinderSupportAutoConfiguration#keyValueSerdeResolver matched:
      - @ConditionalOnMissingBean (types: org.springframework.cloud.stream.binder.kafka.streams.KeyValueSerdeResolver; SearchStrategy: all) did not find any beans (OnBeanCondition)

   KafkaStreamsFunctionAutoConfiguration#kafkaStreamsFunctionBeanPostProcessor matched:
      - Matched. Function/BiFunction/Consumer beans found (FunctionDetectorCondition)

   KafkaStreamsFunctionAutoConfiguration#kafkaStreamsFunctionProcessorInvoker matched:
      - Matched. Function/BiFunction/Consumer beans found (FunctionDetectorCondition)

   LifecycleAutoConfiguration#defaultLifecycleProcessor matched:
      - @ConditionalOnMissingBean (names: lifecycleProcessor; SearchStrategy: all) did not find any beans (OnBeanCondition)

   MultipartAutoConfiguration matched:
      - @ConditionalOnClass found required classes 'javax.servlet.Servlet', 'org.springframework.web.multipart.support.StandardServletMultipartResolver', 'javax.servlet.MultipartConfigElement' (OnClassCondition)
      - found 'session' scope (OnWebApplicationCondition)
      - @ConditionalOnProperty (spring.servlet.multipart.enabled) matched (OnPropertyCondition)

   MultipartAutoConfiguration#multipartConfigElement matched:
      - @ConditionalOnMissingBean (types: javax.servlet.MultipartConfigElement,org.springframework.web.multipart.commons.CommonsMultipartResolver; SearchStrategy: all) did not find any beans (OnBeanCondition)

   MultipartAutoConfiguration#multipartResolver matched:
      - @ConditionalOnMissingBean (types: org.springframework.web.multipart.MultipartResolver; SearchStrategy: all) did not find any beans (OnBeanCondition)

   NoOpCacheConfiguration matched:
      - Cache org.springframework.boot.autoconfigure.cache.NoOpCacheConfiguration automatic cache type (CacheCondition)

   OAuth2ResourceServerAutoConfiguration matched:
      - found 'session' scope (OnWebApplicationCondition)

   PersistenceExceptionTranslationAutoConfiguration matched:
      - @ConditionalOnClass found required class 'org.springframework.dao.annotation.PersistenceExceptionTranslationPostProcessor' (OnClassCondition)

   PersistenceExceptionTranslationAutoConfiguration#persistenceExceptionTranslationPostProcessor matched:
      - @ConditionalOnProperty (spring.dao.exceptiontranslation.enabled) matched (OnPropertyCondition)
      - @ConditionalOnMissingBean (types: org.springframework.dao.annotation.PersistenceExceptionTranslationPostProcessor; SearchStrategy: all) did not find any beans (OnBeanCondition)

   PropertyPlaceholderAutoConfiguration#propertySourcesPlaceholderConfigurer matched:
      - @ConditionalOnMissingBean (types: org.springframework.context.support.PropertySourcesPlaceholderConfigurer; SearchStrategy: current) did not find any beans (OnBeanCondition)

   RestTemplateAutoConfiguration matched:
      - @ConditionalOnClass found required class 'org.springframework.web.client.RestTemplate' (OnClassCondition)
      - NoneNestedConditions 0 matched 1 did not; NestedCondition on RestTemplateAutoConfiguration.NotReactiveWebApplicationCondition.ReactiveWebApplication did not find reactive web application classes (RestTemplateAutoConfiguration.NotReactiveWebApplicationCondition)

   RestTemplateAutoConfiguration#restTemplateBuilder matched:
      - @ConditionalOnMissingBean (types: org.springframework.boot.web.client.RestTemplateBuilder; SearchStrategy: all) did not find any beans (OnBeanCondition)

   ServletWebServerFactoryAutoConfiguration matched:
      - @ConditionalOnClass found required class 'javax.servlet.ServletRequest' (OnClassCondition)
      - found 'session' scope (OnWebApplicationCondition)

   ServletWebServerFactoryAutoConfiguration#tomcatServletWebServerFactoryCustomizer matched:
      - @ConditionalOnClass found required class 'org.apache.catalina.startup.Tomcat' (OnClassCondition)

   ServletWebServerFactoryConfiguration.EmbeddedTomcat matched:
      - @ConditionalOnClass found required classes 'javax.servlet.Servlet', 'org.apache.catalina.startup.Tomcat', 'org.apache.coyote.UpgradeProtocol' (OnClassCondition)
      - @ConditionalOnMissingBean (types: org.springframework.boot.web.servlet.server.ServletWebServerFactory; SearchStrategy: current) did not find any beans (OnBeanCondition)

   SimpleCacheConfiguration matched:
      - Cache org.springframework.boot.autoconfigure.cache.SimpleCacheConfiguration automatic cache type (CacheCondition)

   SpringApplicationAdminJmxAutoConfiguration matched:
      - @ConditionalOnProperty (spring.application.admin.enabled=true) matched (OnPropertyCondition)

   SpringApplicationAdminJmxAutoConfiguration#springApplicationAdminRegistrar matched:
      - @ConditionalOnMissingBean (types: org.springframework.boot.admin.SpringApplicationAdminMXBeanRegistrar; SearchStrategy: all) did not find any beans (OnBeanCondition)

   TaskExecutionAutoConfiguration matched:
      - @ConditionalOnClass found required class 'org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor' (OnClassCondition)

   TaskExecutionAutoConfiguration#applicationTaskExecutor matched:
      - @ConditionalOnMissingBean (types: java.util.concurrent.Executor; SearchStrategy: all) did not find any beans (OnBeanCondition)

   TaskExecutionAutoConfiguration#taskExecutorBuilder matched:
      - @ConditionalOnMissingBean (types: org.springframework.boot.task.TaskExecutorBuilder; SearchStrategy: all) did not find any beans (OnBeanCondition)

   TaskSchedulingAutoConfiguration matched:
      - @ConditionalOnClass found required class 'org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler' (OnClassCondition)

   TaskSchedulingAutoConfiguration#taskSchedulerBuilder matched:
      - @ConditionalOnMissingBean (types: org.springframework.boot.task.TaskSchedulerBuilder; SearchStrategy: all) did not find any beans (OnBeanCondition)

   TransactionAutoConfiguration matched:
      - @ConditionalOnClass found required class 'org.springframework.transaction.PlatformTransactionManager' (OnClassCondition)

   TransactionAutoConfiguration#platformTransactionManagerCustomizers matched:
      - @ConditionalOnMissingBean (types: org.springframework.boot.autoconfigure.transaction.TransactionManagerCustomizers; SearchStrategy: all) did not find any beans (OnBeanCondition)

   ValidationAutoConfiguration matched:
      - @ConditionalOnClass found required class 'javax.validation.executable.ExecutableValidator' (OnClassCondition)
      - @ConditionalOnResource found location classpath:META-INF/services/javax.validation.spi.ValidationProvider (OnResourceCondition)

   ValidationAutoConfiguration#defaultValidator matched:
      - @ConditionalOnMissingBean (types: javax.validation.Validator; SearchStrategy: all) did not find any beans (OnBeanCondition)

   ValidationAutoConfiguration#methodValidationPostProcessor matched:
      - @ConditionalOnMissingBean (types: org.springframework.validation.beanvalidation.MethodValidationPostProcessor; SearchStrategy: all) did not find any beans (OnBeanCondition)

   WebMvcAutoConfiguration matched:
      - @ConditionalOnClass found required classes 'javax.servlet.Servlet', 'org.springframework.web.servlet.DispatcherServlet', 'org.springframework.web.servlet.config.annotation.WebMvcConfigurer' (OnClassCondition)
      - found 'session' scope (OnWebApplicationCondition)
      - @ConditionalOnMissingBean (types: org.springframework.web.servlet.config.annotation.WebMvcConfigurationSupport; SearchStrategy: all) did not find any beans (OnBeanCondition)

   WebMvcAutoConfiguration#formContentFilter matched:
      - @ConditionalOnProperty (spring.mvc.formcontent.filter.enabled) matched (OnPropertyCondition)
      - @ConditionalOnMissingBean (types: org.springframework.web.filter.FormContentFilter; SearchStrategy: all) did not find any beans (OnBeanCondition)

   WebMvcAutoConfiguration.WebMvcAutoConfigurationAdapter#defaultViewResolver matched:
      - @ConditionalOnMissingBean (types: org.springframework.web.servlet.view.InternalResourceViewResolver; SearchStrategy: all) did not find any beans (OnBeanCondition)

   WebMvcAutoConfiguration.WebMvcAutoConfigurationAdapter#requestContextFilter matched:
      - @ConditionalOnMissingBean (types: org.springframework.web.context.request.RequestContextListener,org.springframework.web.filter.RequestContextFilter; SearchStrategy: all) did not find any beans (OnBeanCondition)

   WebMvcAutoConfiguration.WebMvcAutoConfigurationAdapter#viewResolver matched:
      - @ConditionalOnBean (types: org.springframework.web.servlet.ViewResolver; SearchStrategy: all) found beans 'defaultViewResolver', 'beanNameViewResolver', 'mvcViewResolver'; @ConditionalOnMissingBean (names: viewResolver types: org.springframework.web.servlet.view.ContentNegotiatingViewResolver; SearchStrategy: all) did not find any beans (OnBeanCondition)

   WebSocketServletAutoConfiguration matched:
      - @ConditionalOnClass found required classes 'javax.servlet.Servlet', 'javax.websocket.server.ServerContainer' (OnClassCondition)
      - found 'session' scope (OnWebApplicationCondition)

   WebSocketServletAutoConfiguration.TomcatWebSocketConfiguration matched:
      - @ConditionalOnClass found required classes 'org.apache.catalina.startup.Tomcat', 'org.apache.tomcat.websocket.server.WsSci' (OnClassCondition)

   WebSocketServletAutoConfiguration.TomcatWebSocketConfiguration#websocketServletWebServerCustomizer matched:
      - @ConditionalOnMissingBean (names: websocketServletWebServerCustomizer; SearchStrategy: all) did not find any beans (OnBeanCondition)


Negative matches:
-----------------

   ActiveMQAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'javax.jms.ConnectionFactory' (OnClassCondition)

   AopAutoConfiguration.AspectJAutoProxyingConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'org.aspectj.weaver.Advice' (OnClassCondition)

   ArtemisAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'javax.jms.ConnectionFactory' (OnClassCondition)

   BatchAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'org.springframework.batch.core.launch.JobLauncher' (OnClassCondition)

   BindersHealthIndicatorAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'org.springframework.boot.actuate.health.HealthIndicator' (OnClassCondition)

   BindingsEndpointAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'org.springframework.boot.actuate.endpoint.annotation.Endpoint' (OnClassCondition)

   CacheAutoConfiguration:
      Did not match:
         - @ConditionalOnBean (types: org.springframework.cache.interceptor.CacheAspectSupport; SearchStrategy: all) did not find any beans of type org.springframework.cache.interceptor.CacheAspectSupport (OnBeanCondition)
      Matched:
         - @ConditionalOnClass found required class 'org.springframework.cache.CacheManager' (OnClassCondition)

   CacheAutoConfiguration.CacheManagerEntityManagerFactoryDependsOnPostProcessor:
      Did not match:
         - @ConditionalOnClass did not find required class 'org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean' (OnClassCondition)
         - Ancestor org.springframework.boot.autoconfigure.cache.CacheAutoConfiguration did not match (ConditionEvaluationReport.AncestorsMatchedCondition)

   CaffeineCacheConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'com.github.benmanes.caffeine.cache.Caffeine' (OnClassCondition)

   CassandraAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'com.datastax.oss.driver.api.core.CqlSession' (OnClassCondition)

   CassandraDataAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'com.datastax.oss.driver.api.core.CqlSession' (OnClassCondition)

   CassandraReactiveDataAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'com.datastax.oss.driver.api.core.CqlSession' (OnClassCondition)

   CassandraReactiveRepositoriesAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'org.springframework.data.cassandra.ReactiveSession' (OnClassCondition)

   CassandraRepositoriesAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'com.datastax.oss.driver.api.core.CqlSession' (OnClassCondition)

   ChannelsEndpointAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'org.springframework.boot.actuate.endpoint.annotation.Endpoint' (OnClassCondition)

   ClientHttpConnectorAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'org.springframework.web.reactive.function.client.WebClient' (OnClassCondition)

   CodecsAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'org.springframework.web.reactive.function.client.WebClient' (OnClassCondition)

   ContextFunctionCatalogAutoConfiguration.GsonConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'com.google.gson.Gson' (OnClassCondition)

   CouchbaseAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'com.couchbase.client.java.Cluster' (OnClassCondition)

   CouchbaseCacheConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'com.couchbase.client.java.Cluster' (OnClassCondition)

   CouchbaseDataAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'com.couchbase.client.java.Bucket' (OnClassCondition)

   CouchbaseReactiveDataAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'com.couchbase.client.java.Cluster' (OnClassCondition)

   CouchbaseReactiveRepositoriesAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'com.couchbase.client.java.Cluster' (OnClassCondition)

   CouchbaseRepositoriesAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'com.couchbase.client.java.Bucket' (OnClassCondition)

   DataSourceAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'org.springframework.jdbc.datasource.embedded.EmbeddedDatabaseType' (OnClassCondition)

   DataSourceTransactionManagerAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'org.springframework.jdbc.core.JdbcTemplate' (OnClassCondition)

   DestinationPublishingMetricsAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'org.springframework.boot.actuate.autoconfigure.metrics.MetricsAutoConfiguration' (OnClassCondition)

   DispatcherServletAutoConfiguration.DispatcherServletConfiguration#multipartResolver:
      Did not match:
         - @ConditionalOnBean (types: org.springframework.web.multipart.MultipartResolver; SearchStrategy: all) did not find any beans of type org.springframework.web.multipart.MultipartResolver (OnBeanCondition)

   EhCacheCacheConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'net.sf.ehcache.Cache' (OnClassCondition)

   ElasticsearchDataAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'org.springframework.data.elasticsearch.core.ElasticsearchTemplate' (OnClassCondition)

   ElasticsearchRepositoriesAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'org.elasticsearch.client.Client' (OnClassCondition)

   ElasticsearchRestClientAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'org.elasticsearch.client.RestClient' (OnClassCondition)

   EmbeddedLdapAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'com.unboundid.ldap.listener.InMemoryDirectoryServer' (OnClassCondition)

   EmbeddedMongoAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'com.mongodb.MongoClientSettings' (OnClassCondition)

   EmbeddedWebServerFactoryCustomizerAutoConfiguration.JettyWebServerFactoryCustomizerConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required classes 'org.eclipse.jetty.server.Server', 'org.eclipse.jetty.util.Loader', 'org.eclipse.jetty.webapp.WebAppContext' (OnClassCondition)

   EmbeddedWebServerFactoryCustomizerAutoConfiguration.NettyWebServerFactoryCustomizerConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'reactor.netty.http.server.HttpServer' (OnClassCondition)

   EmbeddedWebServerFactoryCustomizerAutoConfiguration.UndertowWebServerFactoryCustomizerConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required classes 'io.undertow.Undertow', 'org.xnio.SslClientAuthMode' (OnClassCondition)

   ErrorWebFluxAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'org.springframework.web.reactive.config.WebFluxConfigurer' (OnClassCondition)

   FlywayAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'org.flywaydb.core.Flyway' (OnClassCondition)

   FreeMarkerAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'freemarker.template.Configuration' (OnClassCondition)

   GroovyTemplateAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'groovy.text.markup.MarkupTemplateEngine' (OnClassCondition)

   GsonAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'com.google.gson.Gson' (OnClassCondition)

   GsonHttpMessageConvertersConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'com.google.gson.Gson' (OnClassCondition)

   H2ConsoleAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'org.h2.server.web.WebServlet' (OnClassCondition)

   HazelcastAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'com.hazelcast.core.HazelcastInstance' (OnClassCondition)

   HazelcastCacheConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'com.hazelcast.core.HazelcastInstance' (OnClassCondition)

   HazelcastJpaDependencyAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'com.hazelcast.core.HazelcastInstance' (OnClassCondition)

   HibernateJpaAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'javax.persistence.EntityManager' (OnClassCondition)

   HttpHandlerAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'org.springframework.web.reactive.DispatcherHandler' (OnClassCondition)

   HypermediaAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'org.springframework.hateoas.EntityModel' (OnClassCondition)

   InfinispanCacheConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'org.infinispan.spring.embedded.provider.SpringEmbeddedCacheManager' (OnClassCondition)

   InfluxDbAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'org.influxdb.InfluxDB' (OnClassCondition)

   IntegrationAutoConfiguration.IntegrationJdbcConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'org.springframework.integration.jdbc.store.JdbcMessageStore' (OnClassCondition)

   IntegrationAutoConfiguration.IntegrationRSocketConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required classes 'org.springframework.integration.rsocket.IntegrationRSocketEndpoint', 'io.rsocket.RSocketFactory' (OnClassCondition)

   JCacheCacheConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'javax.cache.Caching' (OnClassCondition)

   JacksonHttpMessageConvertersConfiguration.MappingJackson2XmlHttpMessageConverterConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'com.fasterxml.jackson.dataformat.xml.XmlMapper' (OnClassCondition)

   JdbcRepositoriesAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'org.springframework.data.jdbc.repository.config.AbstractJdbcConfiguration' (OnClassCondition)

   JdbcTemplateAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'org.springframework.jdbc.core.JdbcTemplate' (OnClassCondition)

   JerseyAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'org.glassfish.jersey.server.spring.SpringComponentProvider' (OnClassCondition)

   JmsAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'javax.jms.Message' (OnClassCondition)

   JndiConnectionFactoryAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'org.springframework.jms.core.JmsTemplate' (OnClassCondition)

   JndiDataSourceAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'org.springframework.jdbc.datasource.embedded.EmbeddedDatabaseType' (OnClassCondition)

   JooqAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'org.jooq.DSLContext' (OnClassCondition)

   JpaRepositoriesAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'org.springframework.data.jpa.repository.JpaRepository' (OnClassCondition)

   JsonbAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'javax.json.bind.Jsonb' (OnClassCondition)

   JsonbHttpMessageConvertersConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'javax.json.bind.Jsonb' (OnClassCondition)

   JtaAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'javax.transaction.Transaction' (OnClassCondition)

   KafkaAutoConfiguration#kafkaJaasInitializer:
      Did not match:
         - @ConditionalOnProperty (spring.kafka.jaas.enabled) did not find property 'spring.kafka.jaas.enabled' (OnPropertyCondition)

   KafkaAutoConfiguration#kafkaTransactionManager:
      Did not match:
         - @ConditionalOnProperty (spring.kafka.producer.transaction-id-prefix) did not find property 'spring.kafka.producer.transaction-id-prefix' (OnPropertyCondition)

   KafkaStreamsAnnotationDrivenConfiguration:
      Did not match:
         - @ConditionalOnBean (names: defaultKafkaStreamsBuilder; SearchStrategy: all) did not find any beans named defaultKafkaStreamsBuilder (OnBeanCondition)
      Matched:
         - @ConditionalOnClass found required class 'org.apache.kafka.streams.StreamsBuilder' (OnClassCondition)

   KafkaStreamsBinderSupportAutoConfiguration.KafkaStreamsBinderMetricsConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'io.micrometer.core.instrument.MeterRegistry' (OnClassCondition)

   KafkaStreamsBinderSupportAutoConfiguration.KafkaStreamsBinderMetricsConfigurationWithMultiBinder:
      Did not match:
         - @ConditionalOnClass did not find required class 'io.micrometer.core.instrument.MeterRegistry' (OnClassCondition)

   LdapAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'org.springframework.ldap.core.ContextSource' (OnClassCondition)

   LdapRepositoriesAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'org.springframework.data.ldap.repository.LdapRepository' (OnClassCondition)

   LiquibaseAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'liquibase.change.DatabaseChange' (OnClassCondition)

   MailSenderAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'javax.mail.internet.MimeMessage' (OnClassCondition)

   MailSenderValidatorAutoConfiguration:
      Did not match:
         - @ConditionalOnSingleCandidate did not find required type 'org.springframework.mail.javamail.JavaMailSenderImpl' (OnBeanCondition)

   MessageSourceAutoConfiguration:
      Did not match:
         - ResourceBundle did not find bundle with basename messages (MessageSourceAutoConfiguration.ResourceBundleCondition)

   MongoAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'com.mongodb.client.MongoClient' (OnClassCondition)

   MongoDataAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'com.mongodb.client.MongoClient' (OnClassCondition)

   MongoReactiveAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'com.mongodb.reactivestreams.client.MongoClient' (OnClassCondition)

   MongoReactiveDataAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'com.mongodb.reactivestreams.client.MongoClient' (OnClassCondition)

   MongoReactiveRepositoriesAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'com.mongodb.reactivestreams.client.MongoClient' (OnClassCondition)

   MongoRepositoriesAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'com.mongodb.client.MongoClient' (OnClassCondition)

   MustacheAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'com.samskivert.mustache.Mustache' (OnClassCondition)

   Neo4jDataAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'org.neo4j.ogm.session.SessionFactory' (OnClassCondition)

   Neo4jRepositoriesAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'org.neo4j.ogm.session.Neo4jSession' (OnClassCondition)

   OAuth2ClientAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'org.springframework.security.config.annotation.web.configuration.EnableWebSecurity' (OnClassCondition)

   Oauth2ResourceServerConfiguration.JwtConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required classes 'org.springframework.security.oauth2.server.resource.authentication.JwtAuthenticationToken', 'org.springframework.security.oauth2.jwt.JwtDecoder' (OnClassCondition)

   Oauth2ResourceServerConfiguration.OpaqueTokenConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required classes 'org.springframework.security.oauth2.server.resource.BearerTokenAuthenticationToken', 'org.springframework.security.oauth2.server.resource.introspection.OpaqueTokenIntrospector' (OnClassCondition)

   ProjectInfoAutoConfiguration#buildProperties:
      Did not match:
         - @ConditionalOnResource did not find resource '${spring.info.build.location:classpath:META-INF/build-info.properties}' (OnResourceCondition)

   ProjectInfoAutoConfiguration#gitProperties:
      Did not match:
         - GitResource did not find git info at classpath:git.properties (ProjectInfoAutoConfiguration.GitResourceAvailableCondition)

   QuartzAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'org.quartz.Scheduler' (OnClassCondition)

   R2dbcAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'io.r2dbc.spi.ConnectionFactory' (OnClassCondition)

   R2dbcDataAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'org.springframework.data.r2dbc.core.DatabaseClient' (OnClassCondition)

   R2dbcRepositoriesAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'io.r2dbc.spi.ConnectionFactory' (OnClassCondition)

   R2dbcTransactionManagerAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'org.springframework.data.r2dbc.connectionfactory.R2dbcTransactionManager' (OnClassCondition)

   RSocketMessagingAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'io.rsocket.RSocketFactory' (OnClassCondition)

   RSocketRequesterAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'io.rsocket.RSocketFactory' (OnClassCondition)

   RSocketSecurityAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'org.springframework.security.rsocket.core.SecuritySocketAcceptorInterceptor' (OnClassCondition)

   RSocketServerAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'io.rsocket.core.RSocketServer' (OnClassCondition)

   RSocketStrategiesAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'io.netty.buffer.PooledByteBufAllocator' (OnClassCondition)

   RabbitAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'com.rabbitmq.client.Channel' (OnClassCondition)

   ReactiveElasticsearchRepositoriesAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'org.springframework.data.elasticsearch.client.reactive.ReactiveElasticsearchClient' (OnClassCondition)

   ReactiveElasticsearchRestClientAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'reactor.netty.http.client.HttpClient' (OnClassCondition)

   ReactiveOAuth2ClientAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'org.springframework.security.config.annotation.web.reactive.EnableWebFluxSecurity' (OnClassCondition)

   ReactiveOAuth2ResourceServerAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'org.springframework.security.config.annotation.web.reactive.EnableWebFluxSecurity' (OnClassCondition)

   ReactiveSecurityAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'org.springframework.security.config.annotation.web.reactive.EnableWebFluxSecurity' (OnClassCondition)

   ReactiveUserDetailsServiceAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'org.springframework.security.authentication.ReactiveAuthenticationManager' (OnClassCondition)

   ReactiveWebServerFactoryAutoConfiguration:
      Did not match:
         - @ConditionalOnWebApplication did not find reactive web application classes (OnWebApplicationCondition)

   RedisAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'org.springframework.data.redis.core.RedisOperations' (OnClassCondition)

   RedisCacheConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'org.springframework.data.redis.connection.RedisConnectionFactory' (OnClassCondition)

   RedisReactiveAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'org.springframework.data.redis.connection.ReactiveRedisConnectionFactory' (OnClassCondition)

   RedisRepositoriesAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'org.springframework.data.redis.repository.configuration.EnableRedisRepositories' (OnClassCondition)

   RepositoryRestMvcAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'org.springframework.data.rest.webmvc.config.RepositoryRestMvcConfiguration' (OnClassCondition)

   Saml2RelyingPartyAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'org.springframework.security.saml2.provider.service.registration.RelyingPartyRegistrationRepository' (OnClassCondition)

   SchemaRegistryClientConfiguration#schemaRegistryClient:
      Did not match:
         - @ConditionalOnMissingBean (types: org.springframework.cloud.schema.registry.client.SchemaRegistryClient; SearchStrategy: all) found beans of type 'org.springframework.cloud.schema.registry.client.SchemaRegistryClient' schemaRegistryClient (OnBeanCondition)

   SecurityAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'org.springframework.security.authentication.DefaultAuthenticationEventPublisher' (OnClassCondition)

   SecurityFilterAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'org.springframework.security.config.http.SessionCreationPolicy' (OnClassCondition)

   SendGridAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'com.sendgrid.SendGrid' (OnClassCondition)

   ServletWebServerFactoryAutoConfiguration#forwardedHeaderFilter:
      Did not match:
         - @ConditionalOnProperty (server.forward-headers-strategy=framework) did not find property 'server.forward-headers-strategy' (OnPropertyCondition)

   ServletWebServerFactoryConfiguration.EmbeddedJetty:
      Did not match:
         - @ConditionalOnClass did not find required classes 'org.eclipse.jetty.server.Server', 'org.eclipse.jetty.util.Loader', 'org.eclipse.jetty.webapp.WebAppContext' (OnClassCondition)

   ServletWebServerFactoryConfiguration.EmbeddedUndertow:
      Did not match:
         - @ConditionalOnClass did not find required classes 'io.undertow.Undertow', 'org.xnio.SslClientAuthMode' (OnClassCondition)

   SessionAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'org.springframework.session.Session' (OnClassCondition)

   SolrAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'org.apache.solr.client.solrj.impl.CloudSolrClient' (OnClassCondition)

   SolrRepositoriesAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'org.apache.solr.client.solrj.SolrClient' (OnClassCondition)

   SpringDataWebAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'org.springframework.data.web.PageableHandlerMethodArgumentResolver' (OnClassCondition)

   TaskSchedulingAutoConfiguration#taskScheduler:
      Did not match:
         - @ConditionalOnBean (names: org.springframework.context.annotation.internalScheduledAnnotationProcessor; SearchStrategy: all) did not find any beans named org.springframework.context.annotation.internalScheduledAnnotationProcessor (OnBeanCondition)

   ThymeleafAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'org.thymeleaf.spring5.SpringTemplateEngine' (OnClassCondition)

   TopologyEndpointAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'org.springframework.boot.actuate.endpoint.annotation.Endpoint' (OnClassCondition)

   TransactionAutoConfiguration#transactionalOperator:
      Did not match:
         - @ConditionalOnSingleCandidate (types: org.springframework.transaction.ReactiveTransactionManager; SearchStrategy: all) did not find any beans (OnBeanCondition)

   TransactionAutoConfiguration.EnableTransactionManagementConfiguration:
      Did not match:
         - @ConditionalOnBean (types: org.springframework.transaction.TransactionManager; SearchStrategy: all) did not find any beans of type org.springframework.transaction.TransactionManager (OnBeanCondition)

   TransactionAutoConfiguration.EnableTransactionManagementConfiguration.CglibAutoProxyConfiguration:
      Did not match:
         - Ancestor org.springframework.boot.autoconfigure.transaction.TransactionAutoConfiguration$EnableTransactionManagementConfiguration did not match (ConditionEvaluationReport.AncestorsMatchedCondition)
      Matched:
         - @ConditionalOnProperty (spring.aop.proxy-target-class=true) matched (OnPropertyCondition)

   TransactionAutoConfiguration.EnableTransactionManagementConfiguration.JdkDynamicAutoProxyConfiguration:
      Did not match:
         - @ConditionalOnProperty (spring.aop.proxy-target-class=false) did not find property 'proxy-target-class' (OnPropertyCondition)
         - Ancestor org.springframework.boot.autoconfigure.transaction.TransactionAutoConfiguration$EnableTransactionManagementConfiguration did not match (ConditionEvaluationReport.AncestorsMatchedCondition)

   TransactionAutoConfiguration.TransactionTemplateConfiguration:
      Did not match:
         - @ConditionalOnSingleCandidate (types: org.springframework.transaction.PlatformTransactionManager; SearchStrategy: all) did not find any beans (OnBeanCondition)

   UserDetailsServiceAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'org.springframework.security.authentication.AuthenticationManager' (OnClassCondition)

   WebClientAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'org.springframework.web.reactive.function.client.WebClient' (OnClassCondition)

   WebFluxAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'org.springframework.web.reactive.config.WebFluxConfigurer' (OnClassCondition)

   WebMvcAutoConfiguration#hiddenHttpMethodFilter:
      Did not match:
         - @ConditionalOnProperty (spring.mvc.hiddenmethod.filter.enabled) did not find property 'enabled' (OnPropertyCondition)

   WebMvcAutoConfiguration.ResourceChainCustomizerConfiguration:
      Did not match:
         - @ConditionalOnEnabledResourceChain did not find class org.webjars.WebJarAssetLocator (OnEnabledResourceChainCondition)

   WebMvcAutoConfiguration.WebMvcAutoConfigurationAdapter#beanNameViewResolver:
      Did not match:
         - @ConditionalOnMissingBean (types: org.springframework.web.servlet.view.BeanNameViewResolver; SearchStrategy: all) found beans of type 'org.springframework.web.servlet.view.BeanNameViewResolver' beanNameViewResolver (OnBeanCondition)

   WebMvcAutoConfiguration.WebMvcAutoConfigurationAdapter#localeResolver:
      Did not match:
         - @ConditionalOnProperty (spring.mvc.locale) did not find property 'locale' (OnPropertyCondition)

   WebServiceTemplateAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'org.springframework.oxm.Marshaller' (OnClassCondition)

   WebServicesAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'org.springframework.ws.transport.http.MessageDispatcherServlet' (OnClassCondition)

   WebSocketMessagingAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'org.springframework.web.socket.config.annotation.WebSocketMessageBrokerConfigurer' (OnClassCondition)

   WebSocketReactiveAutoConfiguration:
      Did not match:
         - @ConditionalOnWebApplication did not find reactive web application classes (OnWebApplicationCondition)

   WebSocketServletAutoConfiguration.JettyWebSocketConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'org.eclipse.jetty.websocket.jsr356.server.deploy.WebSocketServerContainerInitializer' (OnClassCondition)

   WebSocketServletAutoConfiguration.UndertowWebSocketConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'io.undertow.websockets.jsr.Bootstrap' (OnClassCondition)

   XADataSourceAutoConfiguration:
      Did not match:
         - @ConditionalOnClass did not find required class 'javax.transaction.TransactionManager' (OnClassCondition)


Exclusions:
-----------

    None


Unconditional classes:
----------------------

    org.springframework.boot.autoconfigure.context.ConfigurationPropertiesAutoConfiguration

    org.springframework.boot.autoconfigure.context.LifecycleAutoConfiguration

    org.springframework.boot.autoconfigure.context.PropertyPlaceholderAutoConfiguration

    org.springframework.cloud.stream.binder.kafka.config.ExtendedBindingHandlerMappingsProviderConfiguration

    org.springframework.cloud.stream.binder.kafka.streams.function.KafkaStreamsFunctionAutoConfiguration

    org.springframework.boot.autoconfigure.availability.ApplicationAvailabilityAutoConfiguration

    org.springframework.boot.autoconfigure.info.ProjectInfoAutoConfiguration



2020-06-24 17:27:24.078  INFO 18164 --- [main] c.t.c.s.s.i.InventoryApplication         : Started InventoryApplication in 3.938 seconds (JVM running for 5.563)
2020-06-24 17:27:24.078  INFO 18164 --- [sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1] o.a.k.c.c.internals.AbstractCoordinator  : [Consumer clientId=sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1-consumer, groupId=sinkInventory-applicationId] Successfully joined group with generation 33
2020-06-24 17:27:24.080  INFO 18164 --- [sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1-consumer, groupId=sinkInventory-applicationId] Updating assignment with
now assigned partitions: inventory-1, inventory-0
compare with previously owned partitions: 
newly added partitions: inventory-1, inventory-0
revoked partitions: 

2020-06-24 17:27:24.084  INFO 18164 --- [sinkItem-applicationId-244f698d-c580-4d52-90fa-1cf875a0c1cb-StreamThread-1] o.a.k.s.p.i.StreamsPartitionAssignor     : stream-thread [sinkItem-applicationId-244f698d-c580-4d52-90fa-1cf875a0c1cb-StreamThread-1-consumer] Assigned tasks to clients as 
244f698d-c580-4d52-90fa-1cf875a0c1cb=[activeTasks: ([0_0, 0_1]) standbyTasks: ([]) assignedTasks: ([0_0, 0_1]) prevActiveTasks: ([]) prevStandbyTasks: ([]) prevAssignedTasks: ([]) prevOwnedPartitionsByConsumerId: ([]) capacity: 1].
2020-06-24 17:27:24.085  INFO 18164 --- [sinkItem-applicationId-244f698d-c580-4d52-90fa-1cf875a0c1cb-StreamThread-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=sinkItem-applicationId-244f698d-c580-4d52-90fa-1cf875a0c1cb-StreamThread-1-consumer, groupId=sinkItem-applicationId] Finished assignment for group at generation 37: {sinkItem-applicationId-244f698d-c580-4d52-90fa-1cf875a0c1cb-StreamThread-1-consumer-1109bd15-8234-4740-8fd8-983e7f7e7433=Assignment(partitions=[items-0, items-1], userDataSize=48)}
2020-06-24 17:27:24.090  INFO 18164 --- [sinkItem-applicationId-244f698d-c580-4d52-90fa-1cf875a0c1cb-StreamThread-1] o.a.k.c.c.internals.AbstractCoordinator  : [Consumer clientId=sinkItem-applicationId-244f698d-c580-4d52-90fa-1cf875a0c1cb-StreamThread-1-consumer, groupId=sinkItem-applicationId] Successfully joined group with generation 37
2020-06-24 17:27:24.091  INFO 18164 --- [sinkItem-applicationId-244f698d-c580-4d52-90fa-1cf875a0c1cb-StreamThread-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=sinkItem-applicationId-244f698d-c580-4d52-90fa-1cf875a0c1cb-StreamThread-1-consumer, groupId=sinkItem-applicationId] Updating assignment with
now assigned partitions: items-0, items-1
compare with previously owned partitions: 
newly added partitions: items-0, items-1
revoked partitions: 

2020-06-24 17:27:25.077  INFO 18164 --- [sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1-consumer, groupId=sinkInventory-applicationId] Adding newly assigned partitions: inventory-1, inventory-0
2020-06-24 17:27:25.077  INFO 18164 --- [processItem-applicationId-6ed3bdd5-4741-487b-b6da-d1698a76b4a3-StreamThread-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=processItem-applicationId-6ed3bdd5-4741-487b-b6da-d1698a76b4a3-StreamThread-1-consumer, groupId=processItem-applicationId] Adding newly assigned partitions: items-0, items-1
2020-06-24 17:27:25.077  INFO 18164 --- [sinkItem-applicationId-244f698d-c580-4d52-90fa-1cf875a0c1cb-StreamThread-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=sinkItem-applicationId-244f698d-c580-4d52-90fa-1cf875a0c1cb-StreamThread-1-consumer, groupId=sinkItem-applicationId] Adding newly assigned partitions: items-0, items-1
2020-06-24 17:27:25.078  INFO 18164 --- [processItem-applicationId-6ed3bdd5-4741-487b-b6da-d1698a76b4a3-StreamThread-1] o.a.k.s.p.internals.StreamThread         : stream-thread [processItem-applicationId-6ed3bdd5-4741-487b-b6da-d1698a76b4a3-StreamThread-1] State transition from STARTING to PARTITIONS_ASSIGNED
2020-06-24 17:27:25.078  INFO 18164 --- [sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1] o.a.k.s.p.internals.StreamThread         : stream-thread [sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1] State transition from STARTING to PARTITIONS_ASSIGNED
2020-06-24 17:27:25.078  INFO 18164 --- [sinkItem-applicationId-244f698d-c580-4d52-90fa-1cf875a0c1cb-StreamThread-1] o.a.k.s.p.internals.StreamThread         : stream-thread [sinkItem-applicationId-244f698d-c580-4d52-90fa-1cf875a0c1cb-StreamThread-1] State transition from STARTING to PARTITIONS_ASSIGNED
2020-06-24 17:27:25.092  INFO 18164 --- [sinkItem-applicationId-244f698d-c580-4d52-90fa-1cf875a0c1cb-StreamThread-1] i.c.k.s.KafkaAvroSerializerConfig        : KafkaAvroSerializerConfig values: 
	bearer.auth.token = [hidden]
	proxy.port = -1
	schema.reflection = false
	auto.register.schemas = true
	max.schemas.per.subject = 1000
	basic.auth.credentials.source = URL
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	schema.registry.url = [http://localhost:8081]
	basic.auth.user.info = [hidden]
	proxy.host = 
	schema.registry.basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2020-06-24 17:27:25.092  INFO 18164 --- [sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1] i.c.k.s.KafkaAvroSerializerConfig        : KafkaAvroSerializerConfig values: 
	bearer.auth.token = [hidden]
	proxy.port = -1
	schema.reflection = false
	auto.register.schemas = true
	max.schemas.per.subject = 1000
	basic.auth.credentials.source = URL
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	schema.registry.url = [http://localhost:8081]
	basic.auth.user.info = [hidden]
	proxy.host = 
	schema.registry.basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2020-06-24 17:27:25.092  INFO 18164 --- [processItem-applicationId-6ed3bdd5-4741-487b-b6da-d1698a76b4a3-StreamThread-1] i.c.k.s.KafkaAvroSerializerConfig        : KafkaAvroSerializerConfig values: 
	bearer.auth.token = [hidden]
	proxy.port = -1
	schema.reflection = false
	auto.register.schemas = true
	max.schemas.per.subject = 1000
	basic.auth.credentials.source = URL
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	schema.registry.url = [http://localhost:8081]
	basic.auth.user.info = [hidden]
	proxy.host = 
	schema.registry.basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2020-06-24 17:27:25.092  INFO 18164 --- [sinkItem-applicationId-244f698d-c580-4d52-90fa-1cf875a0c1cb-StreamThread-1] i.c.k.s.KafkaAvroDeserializerConfig      : KafkaAvroDeserializerConfig values: 
	bearer.auth.token = [hidden]
	proxy.port = -1
	schema.reflection = false
	auto.register.schemas = true
	max.schemas.per.subject = 1000
	basic.auth.credentials.source = URL
	specific.avro.reader = true
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	schema.registry.url = [http://localhost:8081]
	basic.auth.user.info = [hidden]
	proxy.host = 
	schema.registry.basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2020-06-24 17:27:25.092  INFO 18164 --- [sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1] i.c.k.s.KafkaAvroDeserializerConfig      : KafkaAvroDeserializerConfig values: 
	bearer.auth.token = [hidden]
	proxy.port = -1
	schema.reflection = false
	auto.register.schemas = true
	max.schemas.per.subject = 1000
	basic.auth.credentials.source = URL
	specific.avro.reader = true
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	schema.registry.url = [http://localhost:8081]
	basic.auth.user.info = [hidden]
	proxy.host = 
	schema.registry.basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2020-06-24 17:27:25.092  INFO 18164 --- [processItem-applicationId-6ed3bdd5-4741-487b-b6da-d1698a76b4a3-StreamThread-1] i.c.k.s.KafkaAvroDeserializerConfig      : KafkaAvroDeserializerConfig values: 
	bearer.auth.token = [hidden]
	proxy.port = -1
	schema.reflection = false
	auto.register.schemas = true
	max.schemas.per.subject = 1000
	basic.auth.credentials.source = URL
	specific.avro.reader = true
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	schema.registry.url = [http://localhost:8081]
	basic.auth.user.info = [hidden]
	proxy.host = 
	schema.registry.basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2020-06-24 17:27:25.093  INFO 18164 --- [sinkItem-applicationId-244f698d-c580-4d52-90fa-1cf875a0c1cb-StreamThread-1] i.c.k.s.KafkaAvroSerializerConfig        : KafkaAvroSerializerConfig values: 
	bearer.auth.token = [hidden]
	proxy.port = -1
	schema.reflection = false
	auto.register.schemas = true
	max.schemas.per.subject = 1000
	basic.auth.credentials.source = URL
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	schema.registry.url = [http://localhost:8081]
	basic.auth.user.info = [hidden]
	proxy.host = 
	schema.registry.basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2020-06-24 17:27:25.093  INFO 18164 --- [processItem-applicationId-6ed3bdd5-4741-487b-b6da-d1698a76b4a3-StreamThread-1] i.c.k.s.KafkaAvroSerializerConfig        : KafkaAvroSerializerConfig values: 
	bearer.auth.token = [hidden]
	proxy.port = -1
	schema.reflection = false
	auto.register.schemas = true
	max.schemas.per.subject = 1000
	basic.auth.credentials.source = URL
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	schema.registry.url = [http://localhost:8081]
	basic.auth.user.info = [hidden]
	proxy.host = 
	schema.registry.basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2020-06-24 17:27:25.093  INFO 18164 --- [sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1] i.c.k.s.KafkaAvroSerializerConfig        : KafkaAvroSerializerConfig values: 
	bearer.auth.token = [hidden]
	proxy.port = -1
	schema.reflection = false
	auto.register.schemas = true
	max.schemas.per.subject = 1000
	basic.auth.credentials.source = URL
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	schema.registry.url = [http://localhost:8081]
	basic.auth.user.info = [hidden]
	proxy.host = 
	schema.registry.basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2020-06-24 17:27:25.093  INFO 18164 --- [sinkItem-applicationId-244f698d-c580-4d52-90fa-1cf875a0c1cb-StreamThread-1] i.c.k.s.KafkaAvroDeserializerConfig      : KafkaAvroDeserializerConfig values: 
	bearer.auth.token = [hidden]
	proxy.port = -1
	schema.reflection = false
	auto.register.schemas = true
	max.schemas.per.subject = 1000
	basic.auth.credentials.source = URL
	specific.avro.reader = false
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	schema.registry.url = [http://localhost:8081]
	basic.auth.user.info = [hidden]
	proxy.host = 
	schema.registry.basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2020-06-24 17:27:25.093  INFO 18164 --- [processItem-applicationId-6ed3bdd5-4741-487b-b6da-d1698a76b4a3-StreamThread-1] i.c.k.s.KafkaAvroDeserializerConfig      : KafkaAvroDeserializerConfig values: 
	bearer.auth.token = [hidden]
	proxy.port = -1
	schema.reflection = false
	auto.register.schemas = true
	max.schemas.per.subject = 1000
	basic.auth.credentials.source = URL
	specific.avro.reader = false
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	schema.registry.url = [http://localhost:8081]
	basic.auth.user.info = [hidden]
	proxy.host = 
	schema.registry.basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2020-06-24 17:27:25.093  INFO 18164 --- [sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1] i.c.k.s.KafkaAvroDeserializerConfig      : KafkaAvroDeserializerConfig values: 
	bearer.auth.token = [hidden]
	proxy.port = -1
	schema.reflection = false
	auto.register.schemas = true
	max.schemas.per.subject = 1000
	basic.auth.credentials.source = URL
	specific.avro.reader = false
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	schema.registry.url = [http://localhost:8081]
	basic.auth.user.info = [hidden]
	proxy.host = 
	schema.registry.basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2020-06-24 17:27:25.097  INFO 18164 --- [processItem-applicationId-6ed3bdd5-4741-487b-b6da-d1698a76b4a3-StreamThread-1] i.c.k.s.KafkaAvroSerializerConfig        : KafkaAvroSerializerConfig values: 
	bearer.auth.token = [hidden]
	proxy.port = -1
	schema.reflection = false
	auto.register.schemas = true
	max.schemas.per.subject = 1000
	basic.auth.credentials.source = URL
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	schema.registry.url = [http://localhost:8081]
	basic.auth.user.info = [hidden]
	proxy.host = 
	schema.registry.basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2020-06-24 17:27:25.098  INFO 18164 --- [processItem-applicationId-6ed3bdd5-4741-487b-b6da-d1698a76b4a3-StreamThread-1] i.c.k.s.KafkaAvroDeserializerConfig      : KafkaAvroDeserializerConfig values: 
	bearer.auth.token = [hidden]
	proxy.port = -1
	schema.reflection = false
	auto.register.schemas = true
	max.schemas.per.subject = 1000
	basic.auth.credentials.source = URL
	specific.avro.reader = true
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	schema.registry.url = [http://localhost:8081]
	basic.auth.user.info = [hidden]
	proxy.host = 
	schema.registry.basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2020-06-24 17:27:25.098  INFO 18164 --- [processItem-applicationId-6ed3bdd5-4741-487b-b6da-d1698a76b4a3-StreamThread-1] i.c.k.s.KafkaAvroSerializerConfig        : KafkaAvroSerializerConfig values: 
	bearer.auth.token = [hidden]
	proxy.port = -1
	schema.reflection = false
	auto.register.schemas = true
	max.schemas.per.subject = 1000
	basic.auth.credentials.source = URL
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	schema.registry.url = [http://localhost:8081]
	basic.auth.user.info = [hidden]
	proxy.host = 
	schema.registry.basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2020-06-24 17:27:25.098  INFO 18164 --- [sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1] i.c.k.s.KafkaAvroSerializerConfig        : KafkaAvroSerializerConfig values: 
	bearer.auth.token = [hidden]
	proxy.port = -1
	schema.reflection = false
	auto.register.schemas = true
	max.schemas.per.subject = 1000
	basic.auth.credentials.source = URL
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	schema.registry.url = [http://localhost:8081]
	basic.auth.user.info = [hidden]
	proxy.host = 
	schema.registry.basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2020-06-24 17:27:25.098  INFO 18164 --- [processItem-applicationId-6ed3bdd5-4741-487b-b6da-d1698a76b4a3-StreamThread-1] i.c.k.s.KafkaAvroDeserializerConfig      : KafkaAvroDeserializerConfig values: 
	bearer.auth.token = [hidden]
	proxy.port = -1
	schema.reflection = false
	auto.register.schemas = true
	max.schemas.per.subject = 1000
	basic.auth.credentials.source = URL
	specific.avro.reader = false
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	schema.registry.url = [http://localhost:8081]
	basic.auth.user.info = [hidden]
	proxy.host = 
	schema.registry.basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2020-06-24 17:27:25.099  INFO 18164 --- [sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1] i.c.k.s.KafkaAvroDeserializerConfig      : KafkaAvroDeserializerConfig values: 
	bearer.auth.token = [hidden]
	proxy.port = -1
	schema.reflection = false
	auto.register.schemas = true
	max.schemas.per.subject = 1000
	basic.auth.credentials.source = URL
	specific.avro.reader = true
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	schema.registry.url = [http://localhost:8081]
	basic.auth.user.info = [hidden]
	proxy.host = 
	schema.registry.basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2020-06-24 17:27:25.099  INFO 18164 --- [sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1] i.c.k.s.KafkaAvroSerializerConfig        : KafkaAvroSerializerConfig values: 
	bearer.auth.token = [hidden]
	proxy.port = -1
	schema.reflection = false
	auto.register.schemas = true
	max.schemas.per.subject = 1000
	basic.auth.credentials.source = URL
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	schema.registry.url = [http://localhost:8081]
	basic.auth.user.info = [hidden]
	proxy.host = 
	schema.registry.basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2020-06-24 17:27:25.099  INFO 18164 --- [sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1] i.c.k.s.KafkaAvroDeserializerConfig      : KafkaAvroDeserializerConfig values: 
	bearer.auth.token = [hidden]
	proxy.port = -1
	schema.reflection = false
	auto.register.schemas = true
	max.schemas.per.subject = 1000
	basic.auth.credentials.source = URL
	specific.avro.reader = false
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	schema.registry.url = [http://localhost:8081]
	basic.auth.user.info = [hidden]
	proxy.host = 
	schema.registry.basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2020-06-24 17:27:25.099  INFO 18164 --- [processItem-applicationId-6ed3bdd5-4741-487b-b6da-d1698a76b4a3-StreamThread-1] o.a.k.s.p.internals.StreamThread         : stream-thread [processItem-applicationId-6ed3bdd5-4741-487b-b6da-d1698a76b4a3-StreamThread-1] partition assignment took 21 ms.
	currently assigned active tasks: [0_0, 0_1]
	currently assigned standby tasks: []
	revoked active tasks: []
	revoked standby tasks: []

2020-06-24 17:27:25.099  INFO 18164 --- [sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1] o.a.k.s.p.internals.StreamThread         : stream-thread [sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1] partition assignment took 21 ms.
	currently assigned active tasks: [0_0, 0_1]
	currently assigned standby tasks: []
	revoked active tasks: []
	revoked standby tasks: []

2020-06-24 17:27:25.112  INFO 18164 --- [sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1-consumer, groupId=sinkInventory-applicationId] Found no committed offset for partition inventory-0
2020-06-24 17:27:25.112  INFO 18164 --- [sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1-consumer, groupId=sinkInventory-applicationId] Found no committed offset for partition inventory-0
2020-06-24 17:27:25.120  INFO 18164 --- [processItem-applicationId-6ed3bdd5-4741-487b-b6da-d1698a76b4a3-StreamThread-1] o.a.k.clients.consumer.KafkaConsumer     : [Consumer clientId=processItem-applicationId-6ed3bdd5-4741-487b-b6da-d1698a76b4a3-StreamThread-1-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions
2020-06-24 17:27:25.120  INFO 18164 --- [processItem-applicationId-6ed3bdd5-4741-487b-b6da-d1698a76b4a3-StreamThread-1] o.a.k.s.p.internals.StreamThread         : stream-thread [processItem-applicationId-6ed3bdd5-4741-487b-b6da-d1698a76b4a3-StreamThread-1] State transition from PARTITIONS_ASSIGNED to RUNNING
2020-06-24 17:27:25.121  INFO 18164 --- [processItem-applicationId-6ed3bdd5-4741-487b-b6da-d1698a76b4a3-StreamThread-1] org.apache.kafka.streams.KafkaStreams    : stream-client [processItem-applicationId-6ed3bdd5-4741-487b-b6da-d1698a76b4a3] State transition from REBALANCING to RUNNING
2020-06-24 17:27:25.127  INFO 18164 --- [processItem-applicationId-6ed3bdd5-4741-487b-b6da-d1698a76b4a3-StreamThread-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=processItem-applicationId-6ed3bdd5-4741-487b-b6da-d1698a76b4a3-StreamThread-1-consumer, groupId=processItem-applicationId] Setting offset for partition items-0 to the committed offset FetchPosition{offset=1, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:19093 (id: 1 rack: null)], epoch=0}}
2020-06-24 17:27:25.127  INFO 18164 --- [processItem-applicationId-6ed3bdd5-4741-487b-b6da-d1698a76b4a3-StreamThread-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=processItem-applicationId-6ed3bdd5-4741-487b-b6da-d1698a76b4a3-StreamThread-1-consumer, groupId=processItem-applicationId] Setting offset for partition items-1 to the committed offset FetchPosition{offset=4, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:19094 (id: 2 rack: null)], epoch=0}}
2020-06-24 17:27:25.292  INFO 18164 --- [sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1] o.a.k.s.s.i.RocksDBTimestampedStore      : Opening store inventory-table in regular mode
2020-06-24 17:27:25.353  INFO 18164 --- [sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1] o.a.k.s.s.i.RocksDBTimestampedStore      : Opening store inventory-table in regular mode
2020-06-24 17:27:25.361  INFO 18164 --- [sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1] org.apache.kafka.clients.Metadata        : [Consumer clientId=sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1-restore-consumer, groupId=null] Cluster ID: mxWWohzPSwWJyuH0PA9Ofg
2020-06-24 17:27:25.381  INFO 18164 --- [sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1] o.a.k.clients.consumer.KafkaConsumer     : [Consumer clientId=sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1-restore-consumer, groupId=null] Subscribed to partition(s): sinkInventory-applicationId-inventory-table-changelog-1
2020-06-24 17:27:25.381  INFO 18164 --- [sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1] o.a.k.c.c.internals.SubscriptionState    : [Consumer clientId=sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1-restore-consumer, groupId=null] Seeking to EARLIEST offset of partition sinkInventory-applicationId-inventory-table-changelog-1
2020-06-24 17:27:25.381  INFO 18164 --- [sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1] o.a.k.s.p.i.StoreChangelogReader         : stream-thread [sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1] Restoring task 0_1's state store inventory-table from beginning of the changelog sinkInventory-applicationId-inventory-table-changelog-1 
2020-06-24 17:27:25.386  INFO 18164 --- [sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1] o.a.k.c.c.internals.SubscriptionState    : [Consumer clientId=sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1-restore-consumer, groupId=null] Resetting offset for partition sinkInventory-applicationId-inventory-table-changelog-1 to offset 0.
2020-06-24 17:27:25.419  INFO 18164 --- [sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1] o.a.k.s.s.i.RocksDBTimestampedStore      : Opening store inventory-table in regular mode
2020-06-24 17:27:25.469  INFO 18164 --- [sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1] o.a.k.s.s.i.RocksDBTimestampedStore      : Opening store inventory-table in regular mode
2020-06-24 17:27:25.470  INFO 18164 --- [sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1] o.a.k.s.p.i.StoreChangelogReader         : stream-thread [sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1] Finished restoring all active tasks
2020-06-24 17:27:25.470  INFO 18164 --- [sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1] o.a.k.clients.consumer.KafkaConsumer     : [Consumer clientId=sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions
2020-06-24 17:27:25.472  INFO 18164 --- [sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1] o.a.k.clients.consumer.KafkaConsumer     : [Consumer clientId=sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions
2020-06-24 17:27:25.472  INFO 18164 --- [sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1] o.a.k.clients.consumer.KafkaConsumer     : [Consumer clientId=sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions
2020-06-24 17:27:25.472  INFO 18164 --- [sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1] o.a.k.s.p.internals.StreamThread         : stream-thread [sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1] State transition from PARTITIONS_ASSIGNED to RUNNING
2020-06-24 17:27:25.472  INFO 18164 --- [sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1] org.apache.kafka.streams.KafkaStreams    : stream-client [sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c] State transition from REBALANCING to RUNNING
2020-06-24 17:27:25.475  INFO 18164 --- [sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1-consumer, groupId=sinkInventory-applicationId] Found no committed offset for partition inventory-0
2020-06-24 17:27:25.475  INFO 18164 --- [sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1-consumer, groupId=sinkInventory-applicationId] Setting offset for partition inventory-1 to the committed offset FetchPosition{offset=1, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:19094 (id: 2 rack: null)], epoch=0}}
2020-06-24 17:27:25.487  INFO 18164 --- [kafka-coordinator-heartbeat-thread | sinkInventory-applicationId] o.a.k.c.c.internals.SubscriptionState    : [Consumer clientId=sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1-consumer, groupId=sinkInventory-applicationId] Resetting offset for partition inventory-0 to offset 0.
2020-06-24 17:27:25.620  INFO 18164 --- [sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1] Inventory Processor Logger               : Inventory - Key: 98 | Value: {"itemId": 98, "count": 0}
2020-06-24 17:27:25.667  INFO 18164 --- [sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1] Inventory Processor Logger               : Inventory - Key: 98 | Value: {"itemId": 98, "count": 0}
2020-06-24 17:27:25.675  INFO 18164 --- [sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1] Inventory Processor Logger               : Inventory - Key: 92 | Value: {"itemId": 92, "count": 0}
2020-06-24 17:27:26.101  INFO 18164 --- [sinkItem-applicationId-244f698d-c580-4d52-90fa-1cf875a0c1cb-StreamThread-1] i.c.k.s.KafkaAvroSerializerConfig        : KafkaAvroSerializerConfig values: 
	bearer.auth.token = [hidden]
	proxy.port = -1
	schema.reflection = false
	auto.register.schemas = true
	max.schemas.per.subject = 1000
	basic.auth.credentials.source = URL
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	schema.registry.url = [http://localhost:8081]
	basic.auth.user.info = [hidden]
	proxy.host = 
	schema.registry.basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2020-06-24 17:27:26.101  INFO 18164 --- [sinkItem-applicationId-244f698d-c580-4d52-90fa-1cf875a0c1cb-StreamThread-1] i.c.k.s.KafkaAvroDeserializerConfig      : KafkaAvroDeserializerConfig values: 
	bearer.auth.token = [hidden]
	proxy.port = -1
	schema.reflection = false
	auto.register.schemas = true
	max.schemas.per.subject = 1000
	basic.auth.credentials.source = URL
	specific.avro.reader = true
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	schema.registry.url = [http://localhost:8081]
	basic.auth.user.info = [hidden]
	proxy.host = 
	schema.registry.basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2020-06-24 17:27:26.102  INFO 18164 --- [sinkItem-applicationId-244f698d-c580-4d52-90fa-1cf875a0c1cb-StreamThread-1] i.c.k.s.KafkaAvroSerializerConfig        : KafkaAvroSerializerConfig values: 
	bearer.auth.token = [hidden]
	proxy.port = -1
	schema.reflection = false
	auto.register.schemas = true
	max.schemas.per.subject = 1000
	basic.auth.credentials.source = URL
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	schema.registry.url = [http://localhost:8081]
	basic.auth.user.info = [hidden]
	proxy.host = 
	schema.registry.basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2020-06-24 17:27:26.102  INFO 18164 --- [sinkItem-applicationId-244f698d-c580-4d52-90fa-1cf875a0c1cb-StreamThread-1] i.c.k.s.KafkaAvroDeserializerConfig      : KafkaAvroDeserializerConfig values: 
	bearer.auth.token = [hidden]
	proxy.port = -1
	schema.reflection = false
	auto.register.schemas = true
	max.schemas.per.subject = 1000
	basic.auth.credentials.source = URL
	specific.avro.reader = false
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	schema.registry.url = [http://localhost:8081]
	basic.auth.user.info = [hidden]
	proxy.host = 
	schema.registry.basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2020-06-24 17:27:26.102  INFO 18164 --- [sinkItem-applicationId-244f698d-c580-4d52-90fa-1cf875a0c1cb-StreamThread-1] o.a.k.s.p.internals.StreamThread         : stream-thread [sinkItem-applicationId-244f698d-c580-4d52-90fa-1cf875a0c1cb-StreamThread-1] partition assignment took 1024 ms.
	currently assigned active tasks: [0_0, 0_1]
	currently assigned standby tasks: []
	revoked active tasks: []
	revoked standby tasks: []

2020-06-24 17:27:26.109  INFO 18164 --- [sinkItem-applicationId-244f698d-c580-4d52-90fa-1cf875a0c1cb-StreamThread-1] o.a.k.clients.consumer.KafkaConsumer     : [Consumer clientId=sinkItem-applicationId-244f698d-c580-4d52-90fa-1cf875a0c1cb-StreamThread-1-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions
2020-06-24 17:27:26.109  INFO 18164 --- [sinkItem-applicationId-244f698d-c580-4d52-90fa-1cf875a0c1cb-StreamThread-1] o.a.k.s.p.internals.StreamThread         : stream-thread [sinkItem-applicationId-244f698d-c580-4d52-90fa-1cf875a0c1cb-StreamThread-1] State transition from PARTITIONS_ASSIGNED to RUNNING
2020-06-24 17:27:26.109  INFO 18164 --- [sinkItem-applicationId-244f698d-c580-4d52-90fa-1cf875a0c1cb-StreamThread-1] org.apache.kafka.streams.KafkaStreams    : stream-client [sinkItem-applicationId-244f698d-c580-4d52-90fa-1cf875a0c1cb] State transition from REBALANCING to RUNNING
2020-06-24 17:27:26.112  INFO 18164 --- [sinkItem-applicationId-244f698d-c580-4d52-90fa-1cf875a0c1cb-StreamThread-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=sinkItem-applicationId-244f698d-c580-4d52-90fa-1cf875a0c1cb-StreamThread-1-consumer, groupId=sinkItem-applicationId] Setting offset for partition items-0 to the committed offset FetchPosition{offset=1, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:19093 (id: 1 rack: null)], epoch=0}}
2020-06-24 17:27:26.112  INFO 18164 --- [sinkItem-applicationId-244f698d-c580-4d52-90fa-1cf875a0c1cb-StreamThread-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=sinkItem-applicationId-244f698d-c580-4d52-90fa-1cf875a0c1cb-StreamThread-1-consumer, groupId=sinkItem-applicationId] Setting offset for partition items-1 to the committed offset FetchPosition{offset=4, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:19094 (id: 2 rack: null)], epoch=0}}
2020-06-24 17:27:33.473  INFO 18164 --- [sinkItem-applicationId-244f698d-c580-4d52-90fa-1cf875a0c1cb-StreamThread-1] Inventory Processor Logger               : Item - Key: 92 | Value: {"id": 92, "name": "Men+s+Tech+Shell+Full-Zip", "title": "Men's Tech Shell Full-Zip", "category": "mens_outerwear", "price": 50.2, "description": "A versatile full-zip that you can wear all day long and even to the gym. This technical shell features moisture-wicking fabric, added stretch and a hidden pocket for your smartphone or media player", "image": "data/images/10-15068B.jpg", "largeImage": "data/images/10-15068A.jpg", "features": ["100% polyester.", "Smooth, technical front with textured mesh back.", "Drawstring bottom for adjustable fit.", "Raglan sleeves."]}
2020-06-24 17:27:33.621  INFO 18164 --- [sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1] Inventory Processor Logger               : Inventory - Key: 92 | Value: {"itemId": 92, "count": 0}
2020-06-24 17:27:42.862  INFO 18164 --- [http-nio-9000-exec-2] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring DispatcherServlet 'dispatcherServlet'
2020-06-24 17:27:42.862  INFO 18164 --- [http-nio-9000-exec-2] o.s.web.servlet.DispatcherServlet        : Initializing Servlet 'dispatcherServlet'
2020-06-24 17:27:42.862 DEBUG 18164 --- [http-nio-9000-exec-2] o.s.web.servlet.DispatcherServlet        : Detected StandardServletMultipartResolver
2020-06-24 17:27:42.869 DEBUG 18164 --- [http-nio-9000-exec-2] o.s.web.servlet.DispatcherServlet        : enableLoggingRequestDetails='false': request parameters and headers will be masked to prevent unsafe logging of potentially sensitive data
2020-06-24 17:27:42.869  INFO 18164 --- [http-nio-9000-exec-2] o.s.web.servlet.DispatcherServlet        : Completed initialization in 7 ms
2020-06-24 17:27:42.876 DEBUG 18164 --- [http-nio-9000-exec-2] o.s.web.servlet.DispatcherServlet        : GET "/inventory/item?id=92", parameters={masked}
2020-06-24 17:27:42.878 DEBUG 18164 --- [http-nio-9000-exec-2] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped to com.turkishairlines.concepts.shop.services.inventory.controller.InventoryController#getItemById(Long)
2020-06-24 17:27:42.905 DEBUG 18164 --- [http-nio-9000-exec-2] m.m.a.RequestResponseBodyMethodProcessor : Using 'application/json', given [application/json] and supported [text/plain, */*, text/plain, */*, application/json, application/*+json, application/json, application/*+json]
2020-06-24 17:27:42.905 DEBUG 18164 --- [http-nio-9000-exec-2] m.m.a.RequestResponseBodyMethodProcessor : Writing ["{"itemId": 92, "count": 0}"]
2020-06-24 17:27:42.912 DEBUG 18164 --- [http-nio-9000-exec-2] o.s.web.servlet.DispatcherServlet        : Completed 200 OK
2020-06-24 17:27:50.219 DEBUG 18164 --- [http-nio-9000-exec-5] o.s.web.servlet.DispatcherServlet        : POST "/inventory", parameters={}
2020-06-24 17:27:50.219 DEBUG 18164 --- [http-nio-9000-exec-5] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped to com.turkishairlines.concepts.shop.services.inventory.controller.InventoryController#sendInventoryMessage(Inventory)
2020-06-24 17:27:50.232 DEBUG 18164 --- [http-nio-9000-exec-5] m.m.a.RequestResponseBodyMethodProcessor : Read "application/json;charset=UTF-8" to [{"itemId": 99, "count": 5}]
2020-06-24 17:27:50.237  INFO 18164 --- [http-nio-9000-exec-5] o.a.k.clients.producer.ProducerConfig    : ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:19093, localhost:19094]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = producer-2
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class io.confluent.kafka.serializers.KafkaAvroSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class io.confluent.kafka.serializers.KafkaAvroSerializer

2020-06-24 17:27:50.237  INFO 18164 --- [http-nio-9000-exec-5] i.c.k.s.KafkaAvroSerializerConfig        : KafkaAvroSerializerConfig values: 
	bearer.auth.token = [hidden]
	proxy.port = -1
	schema.reflection = false
	auto.register.schemas = true
	max.schemas.per.subject = 1000
	basic.auth.credentials.source = URL
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	schema.registry.url = [http://localhost:8081]
	basic.auth.user.info = [hidden]
	proxy.host = 
	schema.registry.basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2020-06-24 17:27:50.237  INFO 18164 --- [http-nio-9000-exec-5] i.c.k.s.KafkaAvroSerializerConfig        : KafkaAvroSerializerConfig values: 
	bearer.auth.token = [hidden]
	proxy.port = -1
	schema.reflection = false
	auto.register.schemas = true
	max.schemas.per.subject = 1000
	basic.auth.credentials.source = URL
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	schema.registry.url = [http://localhost:8081]
	basic.auth.user.info = [hidden]
	proxy.host = 
	schema.registry.basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2020-06-24 17:27:50.241  INFO 18164 --- [http-nio-9000-exec-5] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 2.5.0
2020-06-24 17:27:50.241  INFO 18164 --- [http-nio-9000-exec-5] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: 66563e712b0b9f84
2020-06-24 17:27:50.241  INFO 18164 --- [http-nio-9000-exec-5] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1593008870241
2020-06-24 17:27:50.250  INFO 18164 --- [kafka-producer-network-thread | producer-2] org.apache.kafka.clients.Metadata        : [Producer clientId=producer-2] Cluster ID: mxWWohzPSwWJyuH0PA9Ofg
2020-06-24 17:27:50.266  INFO 18164 --- [http-nio-9000-exec-5] c.t.c.s.s.i.source.InventorySource       : Message sent
2020-06-24 17:27:50.267 DEBUG 18164 --- [http-nio-9000-exec-5] m.m.a.RequestResponseBodyMethodProcessor : Using 'application/json', given [application/json] and supported [text/plain, */*, text/plain, */*, application/json, application/*+json, application/json, application/*+json]
2020-06-24 17:27:50.267 DEBUG 18164 --- [http-nio-9000-exec-5] m.m.a.RequestResponseBodyMethodProcessor : Writing ["{"itemId": 99, "count": 5}"]
2020-06-24 17:27:50.270 DEBUG 18164 --- [http-nio-9000-exec-5] o.s.web.servlet.DispatcherServlet        : Completed 200 OK
2020-06-24 17:33:39.000  INFO 18164 --- [RMI TCP Connection(17)-127.0.0.1] inMXBeanRegistrar$SpringApplicationAdmin : Application shutdown requested.
2020-06-24 17:33:39.005 DEBUG 18164 --- [RMI TCP Connection(17)-127.0.0.1] ConfigServletWebServerApplicationContext : Closing org.springframework.boot.web.servlet.context.AnnotationConfigServletWebServerApplicationContext@19e4fcac, started on Wed Jun 24 17:27:20 EET 2020
2020-06-24 17:33:39.343  INFO 18164 --- [RMI TCP Connection(17)-127.0.0.1] org.apache.kafka.streams.KafkaStreams    : stream-client [processItem-applicationId-6ed3bdd5-4741-487b-b6da-d1698a76b4a3] State transition from RUNNING to PENDING_SHUTDOWN
2020-06-24 17:33:39.344  INFO 18164 --- [kafka-streams-close-thread] o.a.k.s.p.internals.StreamThread         : stream-thread [processItem-applicationId-6ed3bdd5-4741-487b-b6da-d1698a76b4a3-StreamThread-1] Informed to shut down
2020-06-24 17:33:39.344  INFO 18164 --- [kafka-streams-close-thread] o.a.k.s.p.internals.StreamThread         : stream-thread [processItem-applicationId-6ed3bdd5-4741-487b-b6da-d1698a76b4a3-StreamThread-1] State transition from RUNNING to PENDING_SHUTDOWN
2020-06-24 17:33:39.395  INFO 18164 --- [processItem-applicationId-6ed3bdd5-4741-487b-b6da-d1698a76b4a3-StreamThread-1] o.a.k.s.p.internals.StreamThread         : stream-thread [processItem-applicationId-6ed3bdd5-4741-487b-b6da-d1698a76b4a3-StreamThread-1] Shutting down
2020-06-24 17:33:39.408  INFO 18164 --- [processItem-applicationId-6ed3bdd5-4741-487b-b6da-d1698a76b4a3-StreamThread-1] o.a.k.clients.consumer.KafkaConsumer     : [Consumer clientId=processItem-applicationId-6ed3bdd5-4741-487b-b6da-d1698a76b4a3-StreamThread-1-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions
2020-06-24 17:33:39.409  INFO 18164 --- [processItem-applicationId-6ed3bdd5-4741-487b-b6da-d1698a76b4a3-StreamThread-1] o.a.k.clients.producer.KafkaProducer     : [Producer clientId=processItem-applicationId-6ed3bdd5-4741-487b-b6da-d1698a76b4a3-StreamThread-1-producer] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2020-06-24 17:33:39.418  INFO 18164 --- [processItem-applicationId-6ed3bdd5-4741-487b-b6da-d1698a76b4a3-StreamThread-1] o.a.k.s.p.internals.StreamThread         : stream-thread [processItem-applicationId-6ed3bdd5-4741-487b-b6da-d1698a76b4a3-StreamThread-1] State transition from PENDING_SHUTDOWN to DEAD
2020-06-24 17:33:39.419  INFO 18164 --- [processItem-applicationId-6ed3bdd5-4741-487b-b6da-d1698a76b4a3-StreamThread-1] o.a.k.s.p.internals.StreamThread         : stream-thread [processItem-applicationId-6ed3bdd5-4741-487b-b6da-d1698a76b4a3-StreamThread-1] Shutdown complete
2020-06-24 17:33:39.421  INFO 18164 --- [kafka-streams-close-thread] org.apache.kafka.streams.KafkaStreams    : stream-client [processItem-applicationId-6ed3bdd5-4741-487b-b6da-d1698a76b4a3] State transition from PENDING_SHUTDOWN to NOT_RUNNING
2020-06-24 17:33:39.422  INFO 18164 --- [RMI TCP Connection(17)-127.0.0.1] org.apache.kafka.streams.KafkaStreams    : stream-client [processItem-applicationId-6ed3bdd5-4741-487b-b6da-d1698a76b4a3] Streams client stopped completely
2020-06-24 17:33:39.422  INFO 18164 --- [RMI TCP Connection(17)-127.0.0.1] org.apache.kafka.streams.KafkaStreams    : stream-client [sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c] State transition from RUNNING to PENDING_SHUTDOWN
2020-06-24 17:33:39.423  INFO 18164 --- [kafka-streams-close-thread] o.a.k.s.p.internals.StreamThread         : stream-thread [sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1] Informed to shut down
2020-06-24 17:33:39.423  INFO 18164 --- [kafka-streams-close-thread] o.a.k.s.p.internals.StreamThread         : stream-thread [sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1] State transition from RUNNING to PENDING_SHUTDOWN
2020-06-24 17:33:39.514  INFO 18164 --- [sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1] o.a.k.s.p.internals.StreamThread         : stream-thread [sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1] Shutting down
2020-06-24 17:33:39.553  INFO 18164 --- [sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1] o.a.k.clients.consumer.KafkaConsumer     : [Consumer clientId=sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions
2020-06-24 17:33:39.554  INFO 18164 --- [sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1] o.a.k.clients.producer.KafkaProducer     : [Producer clientId=sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1-producer] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2020-06-24 17:33:39.561  INFO 18164 --- [sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1] o.a.k.s.p.internals.StreamThread         : stream-thread [sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1] State transition from PENDING_SHUTDOWN to DEAD
2020-06-24 17:33:39.561  INFO 18164 --- [sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1] o.a.k.s.p.internals.StreamThread         : stream-thread [sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c-StreamThread-1] Shutdown complete
2020-06-24 17:33:39.564  INFO 18164 --- [kafka-streams-close-thread] org.apache.kafka.streams.KafkaStreams    : stream-client [sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c] State transition from PENDING_SHUTDOWN to NOT_RUNNING
2020-06-24 17:33:39.564  INFO 18164 --- [RMI TCP Connection(17)-127.0.0.1] org.apache.kafka.streams.KafkaStreams    : stream-client [sinkInventory-applicationId-2081c162-1d3f-4879-b815-f6c07d65a20c] Streams client stopped completely
2020-06-24 17:33:39.567  INFO 18164 --- [RMI TCP Connection(17)-127.0.0.1] o.a.k.s.p.internals.StateDirectory       : stream-thread [RMI TCP Connection(17)-127.0.0.1] Deleting obsolete state directory 0_0 for task 0_0 as 44ms has elapsed (cleanup delay is 0ms).
2020-06-24 17:33:39.579  INFO 18164 --- [RMI TCP Connection(17)-127.0.0.1] o.a.k.s.p.internals.StateDirectory       : stream-thread [RMI TCP Connection(17)-127.0.0.1] Deleting obsolete state directory 0_1 for task 0_1 as 38ms has elapsed (cleanup delay is 0ms).
2020-06-24 17:33:39.593  INFO 18164 --- [RMI TCP Connection(17)-127.0.0.1] org.apache.kafka.streams.KafkaStreams    : stream-client [sinkItem-applicationId-244f698d-c580-4d52-90fa-1cf875a0c1cb] State transition from RUNNING to PENDING_SHUTDOWN
2020-06-24 17:33:39.594  INFO 18164 --- [kafka-streams-close-thread] o.a.k.s.p.internals.StreamThread         : stream-thread [sinkItem-applicationId-244f698d-c580-4d52-90fa-1cf875a0c1cb-StreamThread-1] Informed to shut down
2020-06-24 17:33:39.594  INFO 18164 --- [kafka-streams-close-thread] o.a.k.s.p.internals.StreamThread         : stream-thread [sinkItem-applicationId-244f698d-c580-4d52-90fa-1cf875a0c1cb-StreamThread-1] State transition from RUNNING to PENDING_SHUTDOWN
2020-06-24 17:33:39.627  INFO 18164 --- [sinkItem-applicationId-244f698d-c580-4d52-90fa-1cf875a0c1cb-StreamThread-1] o.a.k.s.p.internals.StreamThread         : stream-thread [sinkItem-applicationId-244f698d-c580-4d52-90fa-1cf875a0c1cb-StreamThread-1] Shutting down
2020-06-24 17:33:39.632  INFO 18164 --- [sinkItem-applicationId-244f698d-c580-4d52-90fa-1cf875a0c1cb-StreamThread-1] o.a.k.clients.consumer.KafkaConsumer     : [Consumer clientId=sinkItem-applicationId-244f698d-c580-4d52-90fa-1cf875a0c1cb-StreamThread-1-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions
2020-06-24 17:33:39.632  INFO 18164 --- [sinkItem-applicationId-244f698d-c580-4d52-90fa-1cf875a0c1cb-StreamThread-1] o.a.k.clients.producer.KafkaProducer     : [Producer clientId=sinkItem-applicationId-244f698d-c580-4d52-90fa-1cf875a0c1cb-StreamThread-1-producer] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2020-06-24 17:33:39.637  INFO 18164 --- [sinkItem-applicationId-244f698d-c580-4d52-90fa-1cf875a0c1cb-StreamThread-1] o.a.k.s.p.internals.StreamThread         : stream-thread [sinkItem-applicationId-244f698d-c580-4d52-90fa-1cf875a0c1cb-StreamThread-1] State transition from PENDING_SHUTDOWN to DEAD
2020-06-24 17:33:39.637  INFO 18164 --- [sinkItem-applicationId-244f698d-c580-4d52-90fa-1cf875a0c1cb-StreamThread-1] o.a.k.s.p.internals.StreamThread         : stream-thread [sinkItem-applicationId-244f698d-c580-4d52-90fa-1cf875a0c1cb-StreamThread-1] Shutdown complete
2020-06-24 17:33:39.639  INFO 18164 --- [kafka-streams-close-thread] org.apache.kafka.streams.KafkaStreams    : stream-client [sinkItem-applicationId-244f698d-c580-4d52-90fa-1cf875a0c1cb] State transition from PENDING_SHUTDOWN to NOT_RUNNING
2020-06-24 17:33:39.640  INFO 18164 --- [RMI TCP Connection(17)-127.0.0.1] org.apache.kafka.streams.KafkaStreams    : stream-client [sinkItem-applicationId-244f698d-c580-4d52-90fa-1cf875a0c1cb] Streams client stopped completely
2020-06-24 17:33:39.643  INFO 18164 --- [RMI TCP Connection(17)-127.0.0.1] o.s.i.endpoint.EventDrivenConsumer       : Removing {logging-channel-adapter:_org.springframework.integration.errorLogger} as a subscriber to the 'errorChannel' channel
2020-06-24 17:33:39.643  INFO 18164 --- [RMI TCP Connection(17)-127.0.0.1] o.s.i.channel.PublishSubscribeChannel    : Channel 'application.errorChannel' has 0 subscriber(s).
2020-06-24 17:33:39.643  INFO 18164 --- [RMI TCP Connection(17)-127.0.0.1] o.s.i.endpoint.EventDrivenConsumer       : stopped bean '_org.springframework.integration.errorLogger'
2020-06-24 17:33:39.645  INFO 18164 --- [RMI TCP Connection(17)-127.0.0.1] o.s.s.c.ThreadPoolTaskScheduler          : Shutting down ExecutorService 'taskScheduler'
2020-06-24 17:33:39.647  INFO 18164 --- [RMI TCP Connection(17)-127.0.0.1] o.s.s.concurrent.ThreadPoolTaskExecutor  : Shutting down ExecutorService 'applicationTaskExecutor'
2020-06-24 17:33:39.649  INFO 18164 --- [RMI TCP Connection(17)-127.0.0.1] o.s.i.monitor.IntegrationMBeanExporter   : Summary on shutdown: bean 'errorChannel'
2020-06-24 17:33:39.649  INFO 18164 --- [RMI TCP Connection(17)-127.0.0.1] o.s.i.monitor.IntegrationMBeanExporter   : Summary on shutdown: bean 'output'
2020-06-24 17:33:39.649  INFO 18164 --- [RMI TCP Connection(17)-127.0.0.1] o.s.i.monitor.IntegrationMBeanExporter   : Summary on shutdown: nullChannel
2020-06-24 17:33:39.649  INFO 18164 --- [RMI TCP Connection(17)-127.0.0.1] o.s.i.monitor.IntegrationMBeanExporter   : Summary on shutdown: bean '_org.springframework.integration.errorLogger.handler' for component '_org.springframework.integration.errorLogger'
